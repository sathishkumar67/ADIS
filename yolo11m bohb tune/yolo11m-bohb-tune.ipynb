{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb2c910",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-29T06:44:47.915190Z",
     "iopub.status.busy": "2025-03-29T06:44:47.914851Z",
     "iopub.status.idle": "2025-03-29T06:44:56.754642Z",
     "shell.execute_reply": "2025-03-29T06:44:56.753645Z"
    },
    "papermill": {
     "duration": 8.845041,
     "end_time": "2025-03-29T06:44:56.756253",
     "exception": false,
     "start_time": "2025-03-29T06:44:47.911212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\r\n",
      "Collecting albumentations\r\n",
      "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting ultralytics\r\n",
      "  Downloading ultralytics-8.3.98-py3-none-any.whl.metadata (37 kB)\r\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.1)\r\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\r\n",
      "Collecting huggingface_hub\r\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\r\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\r\n",
      "Collecting albucore==0.0.23 (from albumentations)\r\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\r\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\r\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\r\n",
      "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\r\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.1)\r\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\r\n",
      "Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\r\n",
      "Downloading ultralytics-8.3.98-py3-none-any.whl (949 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m950.0/950.0 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\r\n",
      "Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: simsimd, huggingface_hub, ultralytics-thop, albucore, ultralytics, albumentations\r\n",
      "  Attempting uninstall: huggingface_hub\r\n",
      "    Found existing installation: huggingface-hub 0.29.0\r\n",
      "    Uninstalling huggingface-hub-0.29.0:\r\n",
      "      Successfully uninstalled huggingface-hub-0.29.0\r\n",
      "  Attempting uninstall: albucore\r\n",
      "    Found existing installation: albucore 0.0.19\r\n",
      "    Uninstalling albucore-0.0.19:\r\n",
      "      Successfully uninstalled albucore-0.0.19\r\n",
      "  Attempting uninstall: albumentations\r\n",
      "    Found existing installation: albumentations 1.4.20\r\n",
      "    Uninstalling albumentations-1.4.20:\r\n",
      "      Successfully uninstalled albumentations-1.4.20\r\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.5 huggingface_hub-0.29.3 simsimd-6.2.1 ultralytics-8.3.98 ultralytics-thop-2.0.14\r\n",
      "Cloning into 'ADIS'...\r\n",
      "remote: Enumerating objects: 929, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (67/67), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\r\n",
      "remote: Total 929 (delta 29), reused 55 (delta 18), pack-reused 862 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (929/929), 6.32 MiB | 33.53 MiB/s, done.\r\n",
      "Resolving deltas: 100% (457/457), done.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U albumentations ultralytics optuna huggingface_hub\n",
    "!git clone https://github.com/sathishkumar67/ADIS.git\n",
    "!mv /kaggle/working/ADIS/* /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "386a203d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T06:44:56.765990Z",
     "iopub.status.busy": "2025-03-29T06:44:56.765711Z",
     "iopub.status.idle": "2025-03-29T06:45:03.593732Z",
     "shell.execute_reply": "2025-03-29T06:45:03.592808Z"
    },
    "papermill": {
     "duration": 6.834487,
     "end_time": "2025-03-29T06:45:03.595508",
     "exception": false,
     "start_time": "2025-03-29T06:44:56.761021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import joblib\n",
    "import optuna\n",
    "from huggingface_hub import hf_hub_download\n",
    "from utils import unzip_file\n",
    "from model import YOLO11Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bf65f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T06:45:03.605567Z",
     "iopub.status.busy": "2025-03-29T06:45:03.605157Z",
     "iopub.status.idle": "2025-03-29T06:45:03.609376Z",
     "shell.execute_reply": "2025-03-29T06:45:03.608752Z"
    },
    "papermill": {
     "duration": 0.010317,
     "end_time": "2025-03-29T06:45:03.610667",
     "exception": false,
     "start_time": "2025-03-29T06:45:03.600350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the global variables\n",
    "REPO_ID = \"pt-sk/ADIS\" \n",
    "FILENAME_IN_REPO = \"dataset.zip\"\n",
    "LOCAL_DIR = os.getcwd()\n",
    "TRAIN_PATH = f\"{LOCAL_DIR}/dataset/train\"\n",
    "VAL_PATH = f\"{LOCAL_DIR}/dataset/val\"\n",
    "TEST_PATH = f\"{LOCAL_DIR}/dataset/test\"\n",
    "DATASET_PATH = f\"{LOCAL_DIR}/{FILENAME_IN_REPO}\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "NUM_CLASSES = 10                                               \n",
    "CLASSES = ['Cat', 'Cattle', 'Chicken', 'Deer', 'Dog', \"Squirrel\", 'Eagle', 'Goat', 'Rodents', 'Snake'] \n",
    "DATA_YAML_FILE = f\"{LOCAL_DIR}/data.yaml\"\n",
    "MODEL_PATH = \"yolo11m.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e83bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T06:45:03.619587Z",
     "iopub.status.busy": "2025-03-29T06:45:03.619364Z",
     "iopub.status.idle": "2025-03-29T06:46:51.531894Z",
     "shell.execute_reply": "2025-03-29T06:46:51.530838Z"
    },
    "papermill": {
     "duration": 107.918595,
     "end_time": "2025-03-29T06:46:51.533397",
     "exception": false,
     "start_time": "2025-03-29T06:45:03.614802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8226ccf2c4bd417bbafdca3f0f2e662c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset.zip:   0%|          | 0.00/9.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.60G/9.60G [00:59<00:00, 162MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted dataset.zip\n"
     ]
    }
   ],
   "source": [
    "# download the dataset and unzip it\n",
    "hf_hub_download(repo_id=REPO_ID, filename=FILENAME_IN_REPO, repo_type=REPO_TYPE, local_dir=LOCAL_DIR)\n",
    "unzip_file(DATASET_PATH, LOCAL_DIR)\n",
    "\n",
    "os.remove(\"/kaggle/working/dataset.zip\")\n",
    "print(\"deleted dataset.zip\")\n",
    "\n",
    "# Get the number of CPU cores\n",
    "num_cores = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4deabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T06:46:51.590283Z",
     "iopub.status.busy": "2025-03-29T06:46:51.589105Z",
     "iopub.status.idle": "2025-03-29T06:46:51.594897Z",
     "shell.execute_reply": "2025-03-29T06:46:51.594161Z"
    },
    "papermill": {
     "duration": 0.03444,
     "end_time": "2025-03-29T06:46:51.596200",
     "exception": false,
     "start_time": "2025-03-29T06:46:51.561760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data yaml file written!.............\n"
     ]
    }
   ],
   "source": [
    "# split paths for model\n",
    "data_yaml = f\"\"\"\n",
    "train: {TRAIN_PATH}\n",
    "val: {VAL_PATH}\n",
    "test: {TEST_PATH}\n",
    "\n",
    "nc: {NUM_CLASSES}\n",
    "names: {CLASSES}\n",
    "\"\"\"\n",
    "\n",
    "# write data yaml file\n",
    "with open(DATA_YAML_FILE, \"w\") as file:\n",
    "    file.write(data_yaml)\n",
    "    print(\"data yaml file written!.............\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec1b314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T06:46:51.656149Z",
     "iopub.status.busy": "2025-03-29T06:46:51.655832Z",
     "iopub.status.idle": "2025-03-29T06:46:51.661944Z",
     "shell.execute_reply": "2025-03-29T06:46:51.661256Z"
    },
    "papermill": {
     "duration": 0.033563,
     "end_time": "2025-03-29T06:46:51.663237",
     "exception": false,
     "start_time": "2025-03-29T06:46:51.629674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    \n",
    "    # Define callback to report intermediate results\n",
    "    def on_train_epoch_end(score, epoch):\n",
    "        trial.report(score, step=epoch)  \n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    callbacks = {\n",
    "        \"on_train_epoch_end\" : on_train_epoch_end\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters using Optuna suggestions\n",
    "    lr0 = trial.suggest_float(\"lr0\", 1e-5, 1e-3, log=True)\n",
    "    lrf = trial.suggest_float(\"lrf\", 0.1, 1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0001, 0.01, log=True)\n",
    "    warmup_momentum = trial.suggest_float(\"warmup_momentum\", 0.5, 0.9)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.8, 0.99)\n",
    "    \n",
    "    CONFIG_DICT = {\n",
    "    \"task\": \"detect\",\n",
    "    \"mode\": \"train\",\n",
    "    \"bohb\": True,\n",
    "    \"custom_callbacks\": callbacks,\n",
    "    \"data\": DATA_YAML_FILE,\n",
    "    \"batch\": 80,\n",
    "    \"imgsz\": 320,\n",
    "    \"save\": True,\n",
    "    \"device\": 0,\n",
    "    \"workers\": num_cores,\n",
    "    \"pretrained\": True,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 5,\n",
    "    \"warmup_epochs\": 1,\n",
    "    \"patience\": 1}\n",
    "\n",
    "    # Train YOLO model\n",
    "    model = YOLO11Model(MODEL_PATH)\n",
    "    model.train(**CONFIG_DICT, lr0=lr0, lrf=lrf, momentum=momentum, weight_decay=weight_decay, warmup_momentum=warmup_momentum)\n",
    "    \n",
    "    # Return validation mAP as the objective value\n",
    "    return model.score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635e5877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T06:46:51.716082Z",
     "iopub.status.busy": "2025-03-29T06:46:51.715796Z",
     "iopub.status.idle": "2025-03-29T09:11:07.107481Z",
     "shell.execute_reply": "2025-03-29T09:11:07.106546Z"
    },
    "papermill": {
     "duration": 8655.420379,
     "end_time": "2025-03-29T09:11:07.109024",
     "exception": false,
     "start_time": "2025-03-29T06:46:51.688645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-29 06:46:51,717] A new study created in memory with name: yolo11_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 38.8M/38.8M [00:00<00:00, 262MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=8.977899112210307e-05, lrf=0.1961596532880056, momentum=0.8570727125067081, weight_decay=0.0034089877501864096, warmup_epochs=1, warmup_momentum=0.7685979707025794, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 46.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 195MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:17<00:00, 1140.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/dataset/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:01<00:00, 1089.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/dataset/val.cache\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=8.977899112210307e-05, momentum=0.8570727125067081) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.004261234687733012), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      11.5G      1.121      1.901      1.296        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1213 | AVG Cls Loss: 1.9006 | AVG DFL Loss: 1.2960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:10<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.718      0.643      0.721       0.47\n",
      "Epoch 1: AVG Val Box Loss: 1.1306 | AVG Val Cls Loss: 1.2987 | AVG Val DFL Loss: 1.2981 | Total Val Loss: 3.7274\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.4G      1.041      1.273      1.269        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.0407 | AVG Cls Loss: 1.2731 | AVG DFL Loss: 1.2693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.817      0.731      0.832      0.581\n",
      "Epoch 2: AVG Val Box Loss: 1.0146 | AVG Val Cls Loss: 0.9798 | AVG Val DFL Loss: 1.1957 | Total Val Loss: 3.1902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.4G     0.9944      1.103      1.244        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 0.9944 | AVG Cls Loss: 1.1029 | AVG DFL Loss: 1.2444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.864      0.826      0.904      0.646\n",
      "Epoch 3: AVG Val Box Loss: 0.9412 | AVG Val Cls Loss: 0.7244 | AVG Val DFL Loss: 1.1388 | Total Val Loss: 2.8043\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      11.4G      0.949     0.9916      1.225        270        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: AVG Box Loss: 0.9490 | AVG Cls Loss: 0.9916 | AVG DFL Loss: 1.2247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.897      0.826      0.914       0.67\n",
      "Epoch 4: AVG Val Box Loss: 0.9012 | AVG Val Cls Loss: 0.6518 | AVG Val DFL Loss: 1.1177 | Total Val Loss: 2.6707\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      11.4G      0.915     0.9082      1.203        227        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: AVG Box Loss: 0.9150 | AVG Cls Loss: 0.9082 | AVG DFL Loss: 1.2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.909      0.876      0.934      0.693\n",
      "Epoch 5: AVG Val Box Loss: 0.8807 | AVG Val Cls Loss: 0.5908 | AVG Val DFL Loss: 1.0934 | Total Val Loss: 2.5649\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.343 hours.\n",
      "\n",
      "Validating Validation Data runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "YOLO11m summary (fused): 214 layers, 24,211,566 parameters, 0 gradients, 87.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:10<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781       0.91      0.875      0.934      0.693\n",
      "                   Cat        140        144      0.919      0.931      0.974      0.784\n",
      "                Cattle        140        174      0.987      0.851      0.955      0.697\n",
      "               Chicken        140        313      0.889      0.853      0.911      0.662\n",
      "                  Deer        140        189       0.94      0.937      0.955      0.754\n",
      "                   Dog        140        169      0.921      0.832       0.93      0.579\n",
      "              Squirrel        140        143       0.92      0.886      0.945      0.722\n",
      "                 Eagle        140        156      0.935      0.917      0.962      0.805\n",
      "                  Goat        140        186       0.84      0.875      0.914      0.692\n",
      "               Rodents        140        155       0.89      0.831      0.915      0.643\n",
      "                 Snake        140        152      0.864      0.835      0.881      0.593\n",
      "Per-class IoU and Accuracy:\n",
      "          Cat: IoU: 0.892 | Accuracy: 0.895\n",
      "          Cattle: IoU: 0.864 | Accuracy: 0.862\n",
      "          Chicken: IoU: 0.858 | Accuracy: 0.860\n",
      "          Deer: IoU: 0.886 | Accuracy: 0.932\n",
      "          Dog: IoU: 0.814 | Accuracy: 0.817\n",
      "          Squirrel: IoU: 0.881 | Accuracy: 0.848\n",
      "          Eagle: IoU: 0.906 | Accuracy: 0.912\n",
      "          Goat: IoU: 0.871 | Accuracy: 0.823\n",
      "          Rodents: IoU: 0.857 | Accuracy: 0.789\n",
      "          Snake: IoU: 0.843 | Accuracy: 0.834\n",
      "          Background Accuracy: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n",
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 3.1ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-29 07:08:37,112] Trial 0 finished with value: 0.6934 and parameters: {'lr0': 8.977899112210307e-05, 'lrf': 0.1961596532880056, 'weight_decay': 0.0034089877501864096, 'warmup_momentum': 0.7685979707025794, 'momentum': 0.8570727125067081}. Best is trial 0 with value: 0.6934.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=1.901655955925459e-05, lrf=0.6690703376996878, momentum=0.8714904125769121, weight_decay=0.009385313858603759, warmup_epochs=1, warmup_momentum=0.5029194731857112, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=1.901655955925459e-05, momentum=0.8714904125769121) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.011731642323254698), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      11.5G      1.115      2.094      1.272        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1153 | AVG Cls Loss: 2.0939 | AVG DFL Loss: 1.2716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.788      0.689      0.775       0.53\n",
      "Epoch 1: AVG Val Box Loss: 0.9946 | AVG Val Cls Loss: 1.1220 | AVG Val DFL Loss: 1.1517 | Total Val Loss: 3.2683\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.5G      1.018      1.338      1.223        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.0176 | AVG Cls Loss: 1.3384 | AVG DFL Loss: 1.2234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.856      0.766      0.865      0.613\n",
      "Epoch 2: AVG Val Box Loss: 0.9611 | AVG Val Cls Loss: 0.8614 | AVG Val DFL Loss: 1.1312 | Total Val Loss: 2.9536\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.5G     0.9815      1.181      1.201        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 0.9815 | AVG Cls Loss: 1.1805 | AVG DFL Loss: 1.2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.868       0.83      0.898      0.651\n",
      "Epoch 3: AVG Val Box Loss: 0.9106 | AVG Val Cls Loss: 0.7458 | AVG Val DFL Loss: 1.0908 | Total Val Loss: 2.7471\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      11.5G     0.9487      1.094      1.188        270        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: AVG Box Loss: 0.9487 | AVG Cls Loss: 1.0938 | AVG DFL Loss: 1.1876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781       0.87      0.823      0.903      0.662\n",
      "Epoch 4: AVG Val Box Loss: 0.9035 | AVG Val Cls Loss: 0.7152 | AVG Val DFL Loss: 1.0905 | Total Val Loss: 2.7093\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      11.5G     0.9349      1.026       1.18        227        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: AVG Box Loss: 0.9349 | AVG Cls Loss: 1.0261 | AVG DFL Loss: 1.1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.897      0.842       0.92      0.681\n",
      "Epoch 5: AVG Val Box Loss: 0.8755 | AVG Val Cls Loss: 0.6638 | AVG Val DFL Loss: 1.0650 | Total Val Loss: 2.6043\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.343 hours.\n",
      "\n",
      "Validating Validation Data runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "YOLO11m summary (fused): 214 layers, 24,211,566 parameters, 0 gradients, 87.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:11<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.897      0.842       0.92      0.681\n",
      "                   Cat        140        144      0.912      0.972      0.984      0.767\n",
      "                Cattle        140        174      0.916      0.868       0.95      0.701\n",
      "               Chicken        140        313      0.886      0.846      0.905      0.647\n",
      "                  Deer        140        189      0.935      0.931      0.962      0.761\n",
      "                   Dog        140        169      0.873      0.834      0.904      0.561\n",
      "              Squirrel        140        143      0.904      0.794      0.907      0.689\n",
      "                 Eagle        140        156      0.923      0.865      0.941      0.796\n",
      "                  Goat        140        186      0.882      0.801      0.892      0.658\n",
      "               Rodents        140        155      0.893      0.703       0.88      0.609\n",
      "                 Snake        140        152      0.848      0.803      0.876      0.619\n",
      "Per-class IoU and Accuracy:\n",
      "          Cat: IoU: 0.894 | Accuracy: 0.898\n",
      "          Cattle: IoU: 0.862 | Accuracy: 0.855\n",
      "          Chicken: IoU: 0.857 | Accuracy: 0.843\n",
      "          Deer: IoU: 0.886 | Accuracy: 0.913\n",
      "          Dog: IoU: 0.806 | Accuracy: 0.772\n",
      "          Squirrel: IoU: 0.871 | Accuracy: 0.787\n",
      "          Eagle: IoU: 0.910 | Accuracy: 0.877\n",
      "          Goat: IoU: 0.874 | Accuracy: 0.759\n",
      "          Rodents: IoU: 0.850 | Accuracy: 0.737\n",
      "          Snake: IoU: 0.849 | Accuracy: 0.833\n",
      "          Background Accuracy: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n",
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 3.1ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-29 07:29:42,110] Trial 1 finished with value: 0.6805 and parameters: {'lr0': 1.901655955925459e-05, 'lrf': 0.6690703376996878, 'weight_decay': 0.009385313858603759, 'warmup_momentum': 0.5029194731857112, 'momentum': 0.8714904125769121}. Best is trial 0 with value: 0.6934.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=2.0225716682321914e-05, lrf=0.22271059779528307, momentum=0.9237378694753409, weight_decay=0.0003440167061423621, warmup_epochs=1, warmup_momentum=0.5226591122338441, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=2.0225716682321914e-05, momentum=0.9237378694753409) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.00043002088267795257), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      11.5G      1.118       2.12      1.272        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1185 | AVG Cls Loss: 2.1200 | AVG DFL Loss: 1.2719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.755      0.681      0.769      0.523\n",
      "Epoch 1: AVG Val Box Loss: 1.0123 | AVG Val Cls Loss: 1.1663 | AVG Val DFL Loss: 1.1589 | Total Val Loss: 3.3375\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.5G      1.019      1.352      1.223        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.0188 | AVG Cls Loss: 1.3520 | AVG DFL Loss: 1.2229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781       0.85      0.784      0.875      0.616\n",
      "Epoch 2: AVG Val Box Loss: 0.9568 | AVG Val Cls Loss: 0.8557 | AVG Val DFL Loss: 1.1361 | Total Val Loss: 2.9486\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.5G     0.9852      1.196      1.204        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 0.9852 | AVG Cls Loss: 1.1964 | AVG DFL Loss: 1.2043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.862      0.842        0.9      0.651\n",
      "Epoch 3: AVG Val Box Loss: 0.9118 | AVG Val Cls Loss: 0.7499 | AVG Val DFL Loss: 1.0987 | Total Val Loss: 2.7604\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      11.5G     0.9538      1.105      1.189        270        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: AVG Box Loss: 0.9538 | AVG Cls Loss: 1.1048 | AVG DFL Loss: 1.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.871      0.842      0.909      0.671\n",
      "Epoch 4: AVG Val Box Loss: 0.8990 | AVG Val Cls Loss: 0.6946 | AVG Val DFL Loss: 1.0946 | Total Val Loss: 2.6882\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      11.5G      0.934      1.047      1.179        227        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: AVG Box Loss: 0.9340 | AVG Cls Loss: 1.0471 | AVG DFL Loss: 1.1788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.885      0.857      0.921      0.684\n",
      "Epoch 5: AVG Val Box Loss: 0.8808 | AVG Val Cls Loss: 0.6522 | AVG Val DFL Loss: 1.0718 | Total Val Loss: 2.6048\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.343 hours.\n",
      "\n",
      "Validating Validation Data runs/detect/train3/weights/best.pt...\n",
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "YOLO11m summary (fused): 214 layers, 24,211,566 parameters, 0 gradients, 87.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:11<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.887      0.857      0.921      0.684\n",
      "                   Cat        140        144      0.907      0.972       0.98      0.786\n",
      "                Cattle        140        174      0.909      0.857      0.949       0.69\n",
      "               Chicken        140        313      0.932      0.815      0.903      0.642\n",
      "                  Deer        140        189      0.915      0.942      0.946      0.748\n",
      "                   Dog        140        169      0.861      0.781      0.902      0.551\n",
      "              Squirrel        140        143      0.887      0.867      0.924       0.71\n",
      "                 Eagle        140        156      0.925      0.859      0.949      0.801\n",
      "                  Goat        140        186      0.829      0.871       0.91      0.683\n",
      "               Rodents        140        155      0.867      0.797      0.874      0.634\n",
      "                 Snake        140        152      0.834      0.809      0.871      0.598\n",
      "Per-class IoU and Accuracy:\n",
      "          Cat: IoU: 0.892 | Accuracy: 0.878\n",
      "          Cattle: IoU: 0.866 | Accuracy: 0.822\n",
      "          Chicken: IoU: 0.854 | Accuracy: 0.842\n",
      "          Deer: IoU: 0.886 | Accuracy: 0.903\n",
      "          Dog: IoU: 0.803 | Accuracy: 0.800\n",
      "          Squirrel: IoU: 0.874 | Accuracy: 0.827\n",
      "          Eagle: IoU: 0.910 | Accuracy: 0.876\n",
      "          Goat: IoU: 0.873 | Accuracy: 0.786\n",
      "          Rodents: IoU: 0.864 | Accuracy: 0.724\n",
      "          Snake: IoU: 0.839 | Accuracy: 0.851\n",
      "          Background Accuracy: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n",
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 3.0ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-29 07:50:48,591] Trial 2 finished with value: 0.6844 and parameters: {'lr0': 2.0225716682321914e-05, 'lrf': 0.22271059779528307, 'weight_decay': 0.0003440167061423621, 'warmup_momentum': 0.5226591122338441, 'momentum': 0.9237378694753409}. Best is trial 0 with value: 0.6934.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0003442001421609707, lrf=0.33762823536077224, momentum=0.9133094234322828, weight_decay=0.000296713331135016, warmup_epochs=1, warmup_momentum=0.5249293465514718, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0003442001421609707, momentum=0.9133094234322828) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.00037089166391877), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      11.5G       1.21      1.997       1.39        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.2102 | AVG Cls Loss: 1.9969 | AVG DFL Loss: 1.3903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.573      0.434      0.486      0.257\n",
      "Epoch 1: AVG Val Box Loss: 1.5920 | AVG Val Cls Loss: 2.4159 | AVG Val DFL Loss: 1.7246 | Total Val Loss: 5.7326\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.5G      1.201      1.612      1.403        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.2013 | AVG Cls Loss: 1.6119 | AVG DFL Loss: 1.4027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.693      0.605      0.668      0.426\n",
      "Epoch 2: AVG Val Box Loss: 1.2348 | AVG Val Cls Loss: 1.4558 | AVG Val DFL Loss: 1.3815 | Total Val Loss: 4.0721\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.5G      1.132      1.397      1.364        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 1.1319 | AVG Cls Loss: 1.3973 | AVG DFL Loss: 1.3644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.735      0.697      0.758      0.494\n",
      "Epoch 3: AVG Val Box Loss: 1.1442 | AVG Val Cls Loss: 1.2099 | AVG Val DFL Loss: 1.3336 | Total Val Loss: 3.6877\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-03-29 08:03:24,206] Trial 3 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=3.3882187593406736e-05, lrf=0.38959513859530603, momentum=0.8701779529432124, weight_decay=0.0020559746500105687, warmup_epochs=1, warmup_momentum=0.8780658243517305, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train5\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train5', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train5/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=3.3882187593406736e-05, momentum=0.8701779529432124) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.002569968312513211), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train5\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      14.9G       1.11      2.003      1.272        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:55<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1100 | AVG Cls Loss: 2.0032 | AVG DFL Loss: 1.2724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.781      0.743       0.81      0.564\n",
      "Epoch 1: AVG Val Box Loss: 0.9915 | AVG Val Cls Loss: 1.0150 | AVG Val DFL Loss: 1.1660 | Total Val Loss: 3.1725\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.5G      1.008      1.272      1.229        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.0084 | AVG Cls Loss: 1.2724 | AVG DFL Loss: 1.2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.836      0.792      0.872       0.62\n",
      "Epoch 2: AVG Val Box Loss: 0.9492 | AVG Val Cls Loss: 0.8463 | AVG Val DFL Loss: 1.1266 | Total Val Loss: 2.9222\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.5G     0.9726       1.12      1.208        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 0.9726 | AVG Cls Loss: 1.1199 | AVG DFL Loss: 1.2077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.877      0.822        0.9      0.653\n",
      "Epoch 3: AVG Val Box Loss: 0.9221 | AVG Val Cls Loss: 0.7313 | AVG Val DFL Loss: 1.0966 | Total Val Loss: 2.7499\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      11.5G     0.9385      1.029      1.191        270        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: AVG Box Loss: 0.9385 | AVG Cls Loss: 1.0288 | AVG DFL Loss: 1.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781       0.89      0.844      0.918      0.678\n",
      "Epoch 4: AVG Val Box Loss: 0.8962 | AVG Val Cls Loss: 0.6651 | AVG Val DFL Loss: 1.0949 | Total Val Loss: 2.6562\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      11.5G     0.9159     0.9592      1.182        227        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:55<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: AVG Box Loss: 0.9159 | AVG Cls Loss: 0.9592 | AVG DFL Loss: 1.1817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.909      0.858      0.933      0.694\n",
      "Epoch 5: AVG Val Box Loss: 0.8717 | AVG Val Cls Loss: 0.6106 | AVG Val DFL Loss: 1.0661 | Total Val Loss: 2.5484\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.344 hours.\n",
      "\n",
      "Validating Validation Data runs/detect/train5/weights/best.pt...\n",
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "YOLO11m summary (fused): 214 layers, 24,211,566 parameters, 0 gradients, 87.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:10<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.909      0.858      0.933      0.694\n",
      "                   Cat        140        144      0.909      0.971      0.975      0.766\n",
      "                Cattle        140        174       0.96      0.828      0.951      0.702\n",
      "               Chicken        140        313       0.94      0.837      0.912      0.659\n",
      "                  Deer        140        189      0.928      0.926       0.96      0.765\n",
      "                   Dog        140        169      0.906      0.805      0.934      0.573\n",
      "              Squirrel        140        143        0.9       0.86       0.94      0.714\n",
      "                 Eagle        140        156      0.913       0.88      0.952      0.813\n",
      "                  Goat        140        186      0.888      0.833      0.914       0.69\n",
      "               Rodents        140        155      0.871      0.783      0.889      0.634\n",
      "                 Snake        140        152      0.879      0.855      0.899       0.62\n",
      "Per-class IoU and Accuracy:\n",
      "          Cat: IoU: 0.893 | Accuracy: 0.892\n",
      "          Cattle: IoU: 0.866 | Accuracy: 0.855\n",
      "          Chicken: IoU: 0.857 | Accuracy: 0.865\n",
      "          Deer: IoU: 0.887 | Accuracy: 0.913\n",
      "          Dog: IoU: 0.810 | Accuracy: 0.819\n",
      "          Squirrel: IoU: 0.874 | Accuracy: 0.855\n",
      "          Eagle: IoU: 0.908 | Accuracy: 0.900\n",
      "          Goat: IoU: 0.878 | Accuracy: 0.816\n",
      "          Rodents: IoU: 0.846 | Accuracy: 0.781\n",
      "          Snake: IoU: 0.850 | Accuracy: 0.834\n",
      "          Background Accuracy: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n",
      "/usr/local/lib/python3.10/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.0ms preprocess, 3.1ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-29 08:24:30,873] Trial 4 finished with value: 0.6935 and parameters: {'lr0': 3.3882187593406736e-05, 'lrf': 0.38959513859530603, 'weight_decay': 0.0020559746500105687, 'warmup_momentum': 0.8780658243517305, 'momentum': 0.8701779529432124}. Best is trial 4 with value: 0.6935.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=5.781614450942615e-05, lrf=0.8344978031815018, momentum=0.9483301881804125, weight_decay=0.00023908888882346447, warmup_epochs=1, warmup_momentum=0.6631991737813079, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train6\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train6', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train6/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=5.781614450942615e-05, momentum=0.9483301881804125) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.00029886111102933057), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train6\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      11.5G      1.131      2.028      1.283        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1311 | AVG Cls Loss: 2.0279 | AVG DFL Loss: 1.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.749      0.682      0.761      0.521\n",
      "Epoch 1: AVG Val Box Loss: 1.0436 | AVG Val Cls Loss: 1.1588 | AVG Val DFL Loss: 1.1896 | Total Val Loss: 3.3921\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-03-29 08:28:54,738] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.00017309919761024538, lrf=0.12680959094491911, momentum=0.8556035250298492, weight_decay=0.0063119900114136695, warmup_epochs=1, warmup_momentum=0.8211837084672763, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train7\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train7', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train7/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00017309919761024538, momentum=0.8556035250298492) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.007889987514267087), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train7\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      14.9G       1.15      1.911      1.346        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1504 | AVG Cls Loss: 1.9113 | AVG DFL Loss: 1.3461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.647      0.553      0.603      0.364\n",
      "Epoch 1: AVG Val Box Loss: 1.2616 | AVG Val Cls Loss: 1.8241 | AVG Val DFL Loss: 1.4220 | Total Val Loss: 4.5077\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.5G        1.1       1.38      1.339        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.1003 | AVG Cls Loss: 1.3804 | AVG DFL Loss: 1.3389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.767      0.671      0.747      0.501\n",
      "Epoch 2: AVG Val Box Loss: 1.1084 | AVG Val Cls Loss: 1.2232 | AVG Val DFL Loss: 1.2778 | Total Val Loss: 3.6094\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.5G      1.043      1.186      1.303        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 1.0429 | AVG Cls Loss: 1.1857 | AVG DFL Loss: 1.3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.867       0.78      0.874      0.607\n",
      "Epoch 3: AVG Val Box Loss: 0.9970 | AVG Val Cls Loss: 0.8287 | AVG Val DFL Loss: 1.1991 | Total Val Loss: 3.0249\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-03-29 08:41:31,765] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0009530827185175077, lrf=0.42783491295065423, momentum=0.8729002887035932, weight_decay=0.004213608779431695, warmup_epochs=1, warmup_momentum=0.8797369482355637, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train8\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train8', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train8/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0009530827185175077, momentum=0.8729002887035932) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.0052670109742896185), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      14.9G      1.349      2.278      1.494        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:55<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.3486 | AVG Cls Loss: 2.2779 | AVG DFL Loss: 1.4937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:10<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.336      0.237      0.152      0.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Val Box Loss: 1.9263 | AVG Val Cls Loss: 3.7249 | AVG Val DFL Loss: 2.4091 | Total Val Loss: 8.0603\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.5G      1.347      2.025      1.502        232        320:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 175/250 [02:44<01:10,  1.07it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "        2/5      11.5G      1.347      2.025      1.502        233        320:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/250 [02:45<01:10,  1.05it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "        2/5      11.5G      1.347      2.024      1.502        268        320:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 176/250 [02:46<01:10,  1.05it/s]  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "        2/5      11.5G      1.347      2.024      1.502        268        320:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 177/250 [02:46<01:11,  1.02it/s]    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "        2/5      11.5G      1.348      2.025      1.503        233        320:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 178/250 [02:47<01:11,  1.01it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "        2/5      11.5G      1.347      2.024      1.502        228        320:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 179/250 [02:48<01:10,  1.01it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f688cc72830>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "        2/5      11.5G      1.334      1.979      1.497        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:55<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.3343 | AVG Cls Loss: 1.9793 | AVG DFL Loss: 1.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.616      0.421      0.458      0.269\n",
      "Epoch 2: AVG Val Box Loss: 1.4333 | AVG Val Cls Loss: 2.1751 | AVG Val DFL Loss: 1.6428 | Total Val Loss: 5.2512\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.5G      1.237      1.696      1.433        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 1.2366 | AVG Cls Loss: 1.6965 | AVG DFL Loss: 1.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.673      0.578      0.638      0.382\n",
      "Epoch 3: AVG Val Box Loss: 1.3444 | AVG Val Cls Loss: 1.4854 | AVG Val DFL Loss: 1.4906 | Total Val Loss: 4.3204\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-03-29 08:54:08,060] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=6.655470940420868e-05, lrf=0.4414422539531147, momentum=0.9426759479225317, weight_decay=0.001047127905487842, warmup_epochs=1, warmup_momentum=0.6646415955881203, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train9', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train9/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=6.655470940420868e-05, momentum=0.9426759479225317) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.0013089098818598024), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train9\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      14.9G      1.128      2.007      1.296        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:55<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1276 | AVG Cls Loss: 2.0066 | AVG DFL Loss: 1.2959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.755      0.689      0.763        0.5\n",
      "Epoch 1: AVG Val Box Loss: 1.1100 | AVG Val Cls Loss: 1.1824 | AVG Val DFL Loss: 1.2848 | Total Val Loss: 3.5771\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-03-29 08:58:31,559] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.98 ğŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mtrainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/kaggle/working/data.yaml, epochs=5, time=None, patience=1, batch=80, imgsz=320, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=train10, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=42, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0002423275694286266, lrf=0.11201054327332864, momentum=0.8864003034757317, weight_decay=0.003388309210773619, warmup_epochs=1, warmup_momentum=0.8360078803536153, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train10\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  blocks.Conv                                  [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  blocks.Conv                                  [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  blocks.C3k2                                  [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  blocks.C3k2                                  [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  blocks.C3k2                                  [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  blocks.SPPF                                  [512, 512, 5]                 \n",
      " 10                  -1  1    990976  blocks.C2PSA                                 [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  blocks.Concat                                [1]                           \n",
      " 13                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  blocks.Concat                                [1]                           \n",
      " 16                  -1  1    542720  blocks.C3k2                                  [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  blocks.Conv                                  [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  blocks.Concat                                [1]                           \n",
      " 19                  -1  1   1511424  blocks.C3k2                                  [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  blocks.Conv                                  [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  blocks.Concat                                [1]                           \n",
      " 22                  -1  1   1642496  blocks.C3k2                                  [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   5590510  blocks.Detect                                [10, [256, 512, 512]]         \n",
      "YOLO11m summary: 314 layers, 24,232,494 parameters, 24,232,478 gradients, 87.8 GFLOPs\n",
      "\n",
      "Transferred 571/613 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train10', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/train.cache... 20000 images, 4 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/val.cache... 1400 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1400/1400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train10/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0002423275694286266, momentum=0.8864003034757317) with parameter groups 100 weight(decay=0.0), 107 weight(decay=0.004235386513467024), 106 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 320 train, 320 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train10\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      14.9G      1.175      1.938      1.364        233        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: AVG Box Loss: 1.1752 | AVG Cls Loss: 1.9381 | AVG DFL Loss: 1.3641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.677      0.546      0.618      0.376\n",
      "Epoch 1: AVG Val Box Loss: 1.2980 | AVG Val Cls Loss: 1.6388 | AVG Val DFL Loss: 1.4808 | Total Val Loss: 4.4176\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      11.5G      1.143      1.471      1.364        244        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: AVG Box Loss: 1.1433 | AVG Cls Loss: 1.4708 | AVG DFL Loss: 1.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:10<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.715      0.655      0.699      0.458\n",
      "Epoch 2: AVG Val Box Loss: 1.1863 | AVG Val Cls Loss: 1.4306 | AVG Val DFL Loss: 1.3321 | Total Val Loss: 3.9489\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      11.5G      1.077      1.257      1.324        232        320: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [03:54<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: AVG Box Loss: 1.0771 | AVG Cls Loss: 1.2566 | AVG DFL Loss: 1.3244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:09<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1400       1781      0.833      0.725      0.821      0.561\n",
      "Epoch 3: AVG Val Box Loss: 1.0488 | AVG Val Cls Loss: 0.9577 | AVG Val DFL Loss: 1.2457 | Total Val Loss: 3.2522\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-03-29 09:11:07,096] Trial 9 pruned. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/optuna_study.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "NUM_TRIALS = 10\n",
    "\n",
    "# load the study\n",
    "study = optuna.create_study(direction='maximize', \n",
    "                            sampler=optuna.samplers.TPESampler(), \n",
    "                            pruner=optuna.pruners.HyperbandPruner(),\n",
    "                            study_name=\"yolo11_tuning\",\n",
    "                            load_if_exists=True)\n",
    "\n",
    "# Optimize with a callback to stop after NUM_TRIALS complete trials\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=NUM_TRIALS)\n",
    "\n",
    "joblib.dump(study, \"/kaggle/working/optuna_study.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065052a9",
   "metadata": {
    "papermill": {
     "duration": 0.968101,
     "end_time": "2025-03-29T09:11:08.956553",
     "exception": false,
     "start_time": "2025-03-29T09:11:07.988452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8788.445156,
   "end_time": "2025-03-29T09:11:13.809170",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-29T06:44:45.364014",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2ec315b0f1ac4f14b977a083b38558a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "32c0b2cc425c4ece87005929a676163c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_afa2c140233c403fbf68a750ac7daf37",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_52a434523c8c4f719f36e2f76d3dd190",
       "tabbable": null,
       "tooltip": null,
       "value": "dataset.zip:â€‡100%"
      }
     },
     "3b1e448181fb4b99ada15403d1201221": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52a434523c8c4f719f36e2f76d3dd190": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8226ccf2c4bd417bbafdca3f0f2e662c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_32c0b2cc425c4ece87005929a676163c",
        "IPY_MODEL_8985c0aaa22943d583a56609413799fa",
        "IPY_MODEL_ee93339057964e3aab051ccbfd483eda"
       ],
       "layout": "IPY_MODEL_3b1e448181fb4b99ada15403d1201221",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8985c0aaa22943d583a56609413799fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cf6956fa7a07469499099362e04ed153",
       "max": 9569453176.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2ec315b0f1ac4f14b977a083b38558a2",
       "tabbable": null,
       "tooltip": null,
       "value": 9569453176.0
      }
     },
     "adb25b3c25e54d70987e97924d804dc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "afa2c140233c403fbf68a750ac7daf37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cf6956fa7a07469499099362e04ed153": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5aa3429d698485986fb1e23bc849ad7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ee93339057964e3aab051ccbfd483eda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_adb25b3c25e54d70987e97924d804dc2",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_e5aa3429d698485986fb1e23bc849ad7",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡9.57G/9.57Gâ€‡[00:47&lt;00:00,â€‡320MB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
