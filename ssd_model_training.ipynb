{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ADIS'...\n",
      "remote: Enumerating objects: 1065, done.\u001b[K\n",
      "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
      "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
      "remote: Total 1065 (delta 45), reused 63 (delta 23), pack-reused 978 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1065/1065), 39.93 MiB | 38.90 MiB/s, done.\n",
      "Resolving deltas: 100% (541/541), done.\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.0.1\n",
      "Collecting ultralytics (from -r requirements.txt (line 1))\n",
      "  Downloading ultralytics-8.3.107-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting albumentations==2.0.5 (from -r requirements.txt (line 2))\n",
      "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: optuna==4.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.2.1)\n",
      "Requirement already satisfied: huggingface-hub==0.30.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.30.2)\n",
      "Collecting lmdb==1.6.2 (from -r requirements.txt (line 5))\n",
      "  Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: msgpack==1.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: opencv-python==4.11.0.86 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.11.0.86)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: pyarrow==19.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (19.0.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (1.26.4)\n",
      "Requirement already satisfied: torchmetrics==1.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.7.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.5->-r requirements.txt (line 2)) (1.15.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.5->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.5->-r requirements.txt (line 2)) (2.11.3)\n",
      "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.5->-r requirements.txt (line 2)) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.5->-r requirements.txt (line 2)) (4.11.0.86)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna==4.2.1->-r requirements.txt (line 3)) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna==4.2.1->-r requirements.txt (line 3)) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna==4.2.1->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna==4.2.1->-r requirements.txt (line 3)) (2.0.38)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna==4.2.1->-r requirements.txt (line 3))\n",
      "  Downloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 4)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 4)) (2025.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 4)) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.30.2->-r requirements.txt (line 4)) (4.13.1)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub==0.30.2->-r requirements.txt (line 4))\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r requirements.txt (line 10)) (1.3.8)\n",
      "Collecting mkl_fft (from numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading mkl_fft-1.3.13-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r requirements.txt (line 10)) (1.2.4)\n",
      "Collecting mkl_random (from numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading mkl_random-1.2.10-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r requirements.txt (line 10)) (0.1.1)\n",
      "Collecting mkl_umath (from numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading mkl_umath-0.1.5-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r requirements.txt (line 10)) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r requirements.txt (line 10)) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r requirements.txt (line 10)) (2.4.1)\n",
      "Collecting mkl-service (from numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading mkl_service-2.4.2-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics==1.7.1->-r requirements.txt (line 11)) (2.5.1+cu124)\n",
      "Collecting torch>=2.0.0 (from torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics==1.7.1->-r requirements.txt (line 11)) (0.14.3)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations==2.0.5->-r requirements.txt (line 2)) (3.11.3)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.23->albumentations==2.0.5->-r requirements.txt (line 2))\n",
      "  Downloading stringzilla-3.12.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (80 kB)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations==2.0.5->-r requirements.txt (line 2)) (6.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics->-r requirements.txt (line 1)) (3.7.5)\n",
      "Collecting matplotlib>=3.3.0 (from ultralytics->-r requirements.txt (line 1))\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics->-r requirements.txt (line 1)) (11.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics->-r requirements.txt (line 1)) (0.20.1+cu124)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics->-r requirements.txt (line 1))\n",
      "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics->-r requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics->-r requirements.txt (line 1)) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics->-r requirements.txt (line 1)) (0.12.2)\n",
      "Collecting seaborn>=0.11.0 (from ultralytics->-r requirements.txt (line 1))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics->-r requirements.txt (line 1))\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna==4.2.1->-r requirements.txt (line 3)) (1.3.9)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna==4.2.1->-r requirements.txt (line 3))\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (75.1.0)\n",
      "Collecting setuptools (from lightning-utilities>=0.8.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1)) (4.56.0)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1))\n",
      "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1)) (3.2.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1))\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.5->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.5->-r requirements.txt (line 2)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.5->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 4)) (2.3.0)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 4))\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.30.2->-r requirements.txt (line 4)) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna==4.2.1->-r requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11))\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->-r requirements.txt (line 10)) (2024.2.0)\n",
      "Collecting intel-openmp<2026,>=2024 (from mkl->numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading intel_openmp-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->-r requirements.txt (line 10)) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->-r requirements.txt (line 10)) (1.2.0)\n",
      "Collecting tcmlib==1.* (from tbb==2022.*->mkl->numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading tcmlib-1.3.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (964 bytes)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4->-r requirements.txt (line 10)) (2024.2.0)\n",
      "Collecting intel-cmplr-lib-rt (from mkl_umath->numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading intel_cmplr_lib_rt-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting intel-cmplr-lib-ur==2025.1.0 (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading intel_cmplr_lib_ur-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting umf==0.10.* (from intel-cmplr-lib-ur==2025.1.0->intel-openmp<2026,>=2024->mkl->numpy==1.26.4->-r requirements.txt (line 10))\n",
      "  Downloading umf-0.10.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics==1.7.1->-r requirements.txt (line 11)) (3.0.2)\n",
      "Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n",
      "Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n",
      "Downloading ultralytics-8.3.107-py3-none-any.whl (974 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.5/974.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading sqlalchemy-2.0.40-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Downloading tcmlib-1.3.0-py2.py3-none-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mkl_fft-1.3.13-0-cp311-cp311-manylinux_2_28_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mkl_random-1.2.10-0-cp311-cp311-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mkl_service-2.4.2-0-cp311-cp311-manylinux_2_28_x86_64.whl (77 kB)\n",
      "Downloading mkl_umath-0.1.5-0-cp311-cp311-manylinux_2_28_x86_64.whl (441 kB)\n",
      "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading intel_openmp-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl (48.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading intel_cmplr_lib_ur-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl (26.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading umf-0.10.0-py2.py3-none-manylinux_2_28_x86_64.whl (314 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading stringzilla-3.12.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (307 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading intel_cmplr_lib_rt-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl (47.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, tcmlib, stringzilla, nvidia-cusparselt-cu12, lmdb, intel-cmplr-lib-rt, urllib3, umf, typing-extensions, setuptools, pyparsing, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, Mako, fonttools, sqlalchemy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, intel-cmplr-lib-ur, nvidia-cusolver-cu12, intel-openmp, torch, mkl-service, mkl_umath, mkl_random, mkl_fft, matplotlib, ultralytics-thop, torchvision, seaborn, ultralytics, albumentations\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.1.0\n",
      "    Uninstalling triton-3.1.0:\n",
      "      Successfully uninstalled triton-3.1.0\n",
      "  Attempting uninstall: tcmlib\n",
      "    Found existing installation: tcmlib 1.2.0\n",
      "    Uninstalling tcmlib-1.2.0:\n",
      "      Successfully uninstalled tcmlib-1.2.0\n",
      "  Attempting uninstall: stringzilla\n",
      "    Found existing installation: stringzilla 3.11.3\n",
      "    Uninstalling stringzilla-3.11.3:\n",
      "      Successfully uninstalled stringzilla-3.11.3\n",
      "  Attempting uninstall: intel-cmplr-lib-rt\n",
      "    Found existing installation: intel-cmplr-lib-rt 2024.2.0\n",
      "    Uninstalling intel-cmplr-lib-rt-2024.2.0:\n",
      "      Successfully uninstalled intel-cmplr-lib-rt-2024.2.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "  Attempting uninstall: umf\n",
      "    Found existing installation: umf 0.9.1\n",
      "    Uninstalling umf-0.9.1:\n",
      "      Successfully uninstalled umf-0.9.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.13.1\n",
      "    Uninstalling typing_extensions-4.13.1:\n",
      "      Successfully uninstalled typing_extensions-4.13.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.1.0\n",
      "    Uninstalling setuptools-75.1.0:\n",
      "      Successfully uninstalled setuptools-75.1.0\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.2.1\n",
      "    Uninstalling pyparsing-3.2.1:\n",
      "      Successfully uninstalled pyparsing-3.2.1\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: Mako\n",
      "    Found existing installation: Mako 1.3.9\n",
      "    Uninstalling Mako-1.3.9:\n",
      "      Successfully uninstalled Mako-1.3.9\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.56.0\n",
      "    Uninstalling fonttools-4.56.0:\n",
      "      Successfully uninstalled fonttools-4.56.0\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.38\n",
      "    Uninstalling SQLAlchemy-2.0.38:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.38\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: intel-cmplr-lib-ur\n",
      "    Found existing installation: intel-cmplr-lib-ur 2024.2.0\n",
      "    Uninstalling intel-cmplr-lib-ur-2024.2.0:\n",
      "      Successfully uninstalled intel-cmplr-lib-ur-2024.2.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "  Attempting uninstall: intel-openmp\n",
      "    Found existing installation: intel-openmp 2024.2.0\n",
      "    Uninstalling intel-openmp-2024.2.0:\n",
      "      Successfully uninstalled intel-openmp-2024.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu124\n",
      "    Uninstalling torch-2.5.1+cu124:\n",
      "      Successfully uninstalled torch-2.5.1+cu124\n",
      "  Attempting uninstall: mkl-service\n",
      "    Found existing installation: mkl-service 2.4.1\n",
      "    Uninstalling mkl-service-2.4.1:\n",
      "      Successfully uninstalled mkl-service-2.4.1\n",
      "  Attempting uninstall: mkl_umath\n",
      "    Found existing installation: mkl-umath 0.1.1\n",
      "    Uninstalling mkl-umath-0.1.1:\n",
      "      Successfully uninstalled mkl-umath-0.1.1\n",
      "  Attempting uninstall: mkl_random\n",
      "    Found existing installation: mkl-random 1.2.4\n",
      "    Uninstalling mkl-random-1.2.4:\n",
      "      Successfully uninstalled mkl-random-1.2.4\n",
      "  Attempting uninstall: mkl_fft\n",
      "    Found existing installation: mkl-fft 1.3.8\n",
      "    Uninstalling mkl-fft-1.3.8:\n",
      "      Successfully uninstalled mkl-fft-1.3.8\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.5\n",
      "    Uninstalling matplotlib-3.7.5:\n",
      "      Successfully uninstalled matplotlib-3.7.5\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1+cu124\n",
      "    Uninstalling torchvision-0.20.1+cu124:\n",
      "      Successfully uninstalled torchvision-0.20.1+cu124\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "  Attempting uninstall: albumentations\n",
      "    Found existing installation: albumentations 2.0.4\n",
      "    Uninstalling albumentations-2.0.4:\n",
      "      Successfully uninstalled albumentations-2.0.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\n",
      "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "ydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.1 which is incompatible.\n",
      "nilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.2 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "ibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "ibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\n",
      "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
      "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.10 albumentations-2.0.5 fonttools-4.57.0 intel-cmplr-lib-rt-2025.1.0 intel-cmplr-lib-ur-2025.1.0 intel-openmp-2025.1.0 lmdb-1.6.2 matplotlib-3.10.1 mkl-service-2.4.2 mkl_fft-1.3.13 mkl_random-1.2.10 mkl_umath-0.1.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 pyparsing-3.2.3 seaborn-0.13.2 setuptools-78.1.0 sqlalchemy-2.0.40 stringzilla-3.12.3 tcmlib-1.3.0 torch-2.6.0 torchvision-0.21.0 triton-3.2.0 typing-extensions-4.13.2 ultralytics-8.3.107 ultralytics-thop-2.0.14 umf-0.10.0 urllib3-2.4.0\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.13)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.10)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.5)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.1.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2025.1.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2025.1.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2025.1.0)\n",
      "Requirement already satisfied: umf==0.10.* in /usr/local/lib/python3.11/dist-packages (from intel-cmplr-lib-ur==2025.1.0->intel-openmp<2026,>=2024->mkl->numpy->torchvision) (0.10.0)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp311-cp311-linux_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1+cu124\n",
      "    Uninstalling torchaudio-2.5.1+cu124:\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu124\n",
      "Successfully installed torchaudio-2.6.0+cu126\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sathishkumar67/ADIS.git\n",
    "!mv /kaggle/working/ADIS/* /kaggle/working/\n",
    "!pip install --upgrade pip\n",
    "!pip install  -r requirements.txt --upgrade --upgrade-strategy eager\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping: 100%|██████████| 7.07G/7.07G [00:43<00:00, 164MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from utils import unzip_file\n",
    "\n",
    "REPO_ID = \"pt-sk/ADIS\" \n",
    "DATASET_NAME = \"balanced_dataset\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "FILENAME_IN_REPO = f\"{DATASET_NAME}.zip\"\n",
    "LOCAL_DIR = os.getcwd()\n",
    "DATASET_PATH = f\"{LOCAL_DIR}/{FILENAME_IN_REPO}\"\n",
    "DATASET_FOLDER_PATH = f\"{LOCAL_DIR}/{DATASET_NAME}\"\n",
    "NUM_CLASSES = 10                                               \n",
    "CLASSES = ['Cat', 'Cattle', 'Chicken', 'Deer', 'Dog', 'Squirrel', 'Eagle', 'Goat', 'Rodents', 'Snake'] \n",
    "BACKGROUND_CLASS_ID = 0\n",
    "MODEL_NUM_CLASSES = NUM_CLASSES + 1     # 1 for background class\n",
    "\n",
    "# download the dataset and unzip it\n",
    "hf_hub_download(repo_id=REPO_ID, filename=FILENAME_IN_REPO, repo_type=REPO_TYPE, local_dir=LOCAL_DIR)\n",
    "unzip_file(DATASET_PATH, LOCAL_DIR)\n",
    "\n",
    "# remove dataset.zip\n",
    "os.remove(DATASET_PATH)\n",
    "\n",
    "# number of cores\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import msgpack\n",
    "import os\n",
    "import cv2\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import shutil\n",
    "from functools import partial\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.detection import _utils as det_utils\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from typing import List, Tuple, Dict, Any # Import necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDLITEOBJDET_DATASET(Dataset):\n",
    "    def __init__(self, root_dir: str, split: str, num_classes: int, img_size: int=320, mode: str=\"train\", dtype=np.float32) -> None:\n",
    "        super().__init__()\n",
    "        self.root_dir, self.split, self.img_size, self.num_classes = root_dir, split.lower(), img_size, num_classes\n",
    "        self.current_dir = os.path.join(self.root_dir, self.split)\n",
    "        self.mode =  mode.lower()\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # check if model is train or eval\n",
    "        if self.mode not in [\"train\", \"eval\"]:\n",
    "            raise ValueError(f\"Invalid mode: {self.mode}. Expected 'train' or 'eval'.\")\n",
    "        \n",
    "        # set interpolation method for resizing\n",
    "        self.interpolation = cv2.INTER_LANCZOS4 if self.mode == \"train\" else cv2.INTER_LINEAR\n",
    "\n",
    "        # Validate current directory\n",
    "        if not os.path.exists(self.current_dir):\n",
    "            raise FileNotFoundError(f\"{self.current_dir} does not exist.\")\n",
    "        elif not os.path.isdir(self.current_dir):\n",
    "            raise NotADirectoryError(f\"{self.current_dir} is not a directory.\")\n",
    "        \n",
    "        # check if the split directory is empty\n",
    "        if len(os.listdir(self.current_dir)) == 0:\n",
    "            raise ValueError(f\"The directory {self.current_dir} is empty.\")\n",
    "        \n",
    "        # get image and label files\n",
    "        self.image_files = sorted(\n",
    "            [os.path.join(self.current_dir, f) for f in os.listdir(self.current_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))],\n",
    "            key=lambda x: os.path.splitext(x)[0]\n",
    "        )\n",
    "        self.label_files = [os.path.join(self.current_dir, os.path.splitext(f)[0] + '.txt') for f in self.image_files]\n",
    "\n",
    "        # Validate existence for ALL label files\n",
    "        for img_file, lbl_file in zip(self.image_files, self.label_files):\n",
    "            if not os.path.exists(lbl_file):\n",
    "                raise FileNotFoundError(f\"Label file missing for {img_file}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # print the no of items in the dataset\n",
    "        print(f\"Number of items in the dataset: {len(self.image_files)}\")\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, dict]:\n",
    "        img_path, label_path = self.image_files[idx], self.label_files[idx]\n",
    "\n",
    "        # Read image and convert to RGB format\n",
    "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        orig_height, orig_width, _ = image.shape\n",
    "        # tensor with uint8 datatype \n",
    "        image = cv2.resize(image, (self.img_size, self.img_size), interpolation=self.interpolation)\n",
    "        \n",
    "        # Read label file and parse the bounding boxes and labels\n",
    "        data = np.loadtxt(label_path, dtype=self.dtype, delimiter=' ', ndmin=2)\n",
    "        \n",
    "        if data.size == 0:\n",
    "            return image, {\n",
    "                'boxes': np.array([[0.0, 0.0, 1.0, 1.0]], dtype=self.dtype),\n",
    "                'labels': np.array(0, dtype=np.uint8)\n",
    "            }\n",
    "        else:\n",
    "            # Convert normalized box coordinates into absolute coordinates, where orig_width and orig_height are your original dimensions.\n",
    "            cx, cy, w, h = data[:, 1], data[:, 2], data[:, 3], data[:, 4]\n",
    "            xmin = np.maximum(0, (cx - w/2) * orig_width)\n",
    "            ymin = np.maximum(0, (cy - h/2) * orig_height)\n",
    "            xmax = np.minimum(orig_width, (cx + w/2) * orig_width)\n",
    "            ymax = np.minimum(orig_height, (cy + h/2) * orig_height)\n",
    "            \n",
    "            # Filter degenerate boxes (width or height less than 1)\n",
    "            valid_mask = ((xmax - xmin) >= 1) & ((ymax - ymin) >= 1)\n",
    "            valid_boxes = np.stack([xmin[valid_mask], ymin[valid_mask],\n",
    "                                    xmax[valid_mask], ymax[valid_mask]], axis=1)\n",
    "\n",
    "            # Adjust class IDs (cid from first column)\n",
    "            valid_labels = data[valid_mask, 0].astype(np.uint8) \n",
    "            np.add(valid_labels, 1, out=valid_labels)  # Increment class IDs by 1 for background class\n",
    "\n",
    "            # scale boxes to new image size\n",
    "            scale_factors = np.array([self.img_size / orig_width, self.img_size / orig_height,\n",
    "                                    self.img_size / orig_width, self.img_size / orig_height], dtype=valid_boxes.dtype)\n",
    "            np.multiply(valid_boxes, scale_factors, out=valid_boxes)\n",
    "            \n",
    "            # Validate class IDs\n",
    "            if np.all((valid_labels < 0) & (valid_labels >= self.num_classes)):\n",
    "                raise ValueError(f\"Invalid class ID in {label_path}\")\n",
    "\n",
    "            if self.mode == \"train\":\n",
    "                np.divide(valid_boxes, self.img_size, out=valid_boxes)\n",
    "\n",
    "            return image, {\n",
    "                'boxes': valid_boxes,\n",
    "                'labels': valid_labels}\n",
    "        \n",
    "    def denormalize(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        # Denormalize boxes to original size\n",
    "        return boxes * self.img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedSSDLITEOBJDET_DATASET(Dataset):\n",
    "    def __init__(self, dataset_class :SSDLITEOBJDET_DATASET, \n",
    "                root_dir: str, \n",
    "                split: str, \n",
    "                num_classes: int, \n",
    "                img_size: int=320, \n",
    "                dtype: np.dtype=np.float32, \n",
    "                mode: str=\"train\",\n",
    "                lmdb_path: str = None,\n",
    "                map_size: int=1099511627776) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.root_dir, self.split, self.img_size, self.num_classes = root_dir, split.lower(), img_size, num_classes\n",
    "        self.dtype = dtype\n",
    "        self.mode = mode.lower()\n",
    "        self.dataset_class = dataset_class\n",
    "        self.map_size = map_size\n",
    "        self.lmdb_path = lmdb_path if lmdb_path else os.path.join(self.root_dir, f\"{self.split}_cache\")\n",
    "        \n",
    "        # preprocess the dataset and cache it in lmdb\n",
    "        self.preprocess_dataset()\n",
    "        \n",
    "        self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False)\n",
    "        with self.env.begin() as txn:\n",
    "            self.length = txn.stat()['entries']\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with self.env.begin() as txn:\n",
    "            data = txn.get(str(idx).encode())\n",
    "        return pickle.loads(data)\n",
    "    \n",
    "    \n",
    "    def preprocess_dataset(self) -> None:\n",
    "        dataset = self.dataset_class(root_dir=self.root_dir,\n",
    "                                    split=self.split, \n",
    "                                    num_classes=self.num_classes, \n",
    "                                    img_size=self.img_size, \n",
    "                                    dtype=self.dtype, \n",
    "                                    mode=self.mode)\n",
    "        # Create LMDB environment\n",
    "        env = lmdb.open(self.lmdb_path, map_size=self.map_size)  # 1TB\n",
    "        \n",
    "        with env.begin(write=True) as txn:\n",
    "            for idx in tqdm(range(len(dataset))):\n",
    "                image, target = dataset[idx]\n",
    "                \n",
    "                # ensure image is contiguous \n",
    "                image = np.ascontiguousarray(image)\n",
    "                boxes = np.ascontiguousarray(target['boxes'])\n",
    "                labels = np.ascontiguousarray(target['labels'])\n",
    "                \n",
    "                # Serialize and store\n",
    "                txn.put(\n",
    "                    str(idx).encode(),\n",
    "                    pickle.dumps((image, boxes, labels))\n",
    "                )\n",
    "\n",
    "        shutil.rmtree(os.path.join(self.root_dir, self.split))\n",
    "        del dataset\n",
    "    \n",
    "    def denormalize(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        # Denormalize boxes to original size\n",
    "        return boxes * self.img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the dataset: 2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2390/2390 [00:29<00:00, 79.72it/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = CachedSSDLITEOBJDET_DATASET(SSDLITEOBJDET_DATASET,\n",
    "                                        root_dir=DATASET_FOLDER_PATH, \n",
    "                                        split='train', \n",
    "                                        num_classes=MODEL_NUM_CLASSES, \n",
    "                                        img_size=320, \n",
    "                                        dtype=np.float32, \n",
    "                                        mode='train')\n",
    "\n",
    "test_dataset = CachedSSDLITEOBJDET_DATASET(SSDLITEOBJDET_DATASET,\n",
    "                                        root_dir=DATASET_FOLDER_PATH, \n",
    "                                        split='test', \n",
    "                                        num_classes=MODEL_NUM_CLASSES, \n",
    "                                        img_size=320, \n",
    "                                        dtype=np.float32, \n",
    "                                        mode='train')\n",
    "\n",
    "val_dataset = CachedSSDLITEOBJDET_DATASET(SSDLITEOBJDET_DATASET,\n",
    "                                        root_dir=DATASET_FOLDER_PATH, \n",
    "                                        split='val', \n",
    "                                        num_classes=MODEL_NUM_CLASSES, \n",
    "                                        img_size=320, \n",
    "                                        dtype=np.float32, \n",
    "                                        mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, drop_last=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, boxes, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for img, boxes, labels in train_loader:\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class AsyncPrefetcher:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.pool = ThreadPoolExecutor(max_workers=4)\n",
    "        self.futures = []\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Prefetch next 4 batches\n",
    "        for _ in range(4):\n",
    "            self.futures.append(self.pool.submit(next, self.dataloader))\n",
    "        while True:\n",
    "            yield self.futures.pop(0).result()\n",
    "            self.futures.append(self.pool.submit(next, self.dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "\n",
    "# Bypass CPU copy using NVIDIA Magnum IO\n",
    "torch.utils.data.DataLoader(..., direct_io=True)\n",
    "\n",
    "# Mount filesystem with GPU Direct support\n",
    "mount -t gpfs /dev/nvme0n1 /datasets\n",
    "\n",
    "# fused batch collation\n",
    "@torch.jit.script\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return torch.stack(images, dim=0, memory_format=torch.channels_last), targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. dataloader configuration\n",
    "2. parallelize label parsing\n",
    "3. use nvidia dali for gpu based dataloading\n",
    "4. use concurrent features in preprocess dataset to speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the dataset: 2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2390/2390 [00:30<00:00, 79.13it/s] \n"
     ]
    }
   ],
   "source": [
    "val_dataset = CachedSSDLITEOBJDET_DATASET(SSDLITEOBJDET_DATASET,\n",
    "                                        DATASET_FOLDER_PATH,\n",
    "                                        \"val\", \n",
    "                                        MODEL_NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "class SSD_MOBILENET_V3_Large(nn.Module):\n",
    "    def __init__(self, num_classes_with_bg:int) -> None:\n",
    "        super(SSD_MOBILENET_V3_Large, self).__init__()\n",
    "        self.num_classes_with_bg = num_classes_with_bg\n",
    "        self.model = ssdlite320_mobilenet_v3_large(weights='COCO_V1', weights_backbone=\"DEFAULT\") \n",
    "        self.model.head.classification_head = SSDLiteClassificationHead(\n",
    "            in_channels=det_utils.retrieve_out_channels(self.model.backbone, (320, 320)),\n",
    "            num_anchors=self.model.anchor_generator.num_anchors_per_location(),\n",
    "            num_classes=self.num_classes_with_bg,\n",
    "            norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.03)\n",
    "        )\n",
    "        self.model.detections_per_img = 100\n",
    "    \n",
    "    def configure_optimizers(self, lr: float = 0.0001, betas: Tuple[float, float] = (0.9, 0.999), weight_decay: float = 0.0001, eps: float = 1e-08, fused: bool = True) -> torch.optim.Optimizer:        \n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for _, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for _, p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        # Create AdamW optimizer and use the fused version if available \n",
    "        return torch.optim.AdamW([{'params': decay_params, 'weight_decay': weight_decay},\n",
    "                                    {'params': nodecay_params, 'weight_decay': 0.0}], \n",
    "                                    lr=lr, \n",
    "                                    betas=betas, \n",
    "                                    eps=eps, \n",
    "                                    fused=fused)\n",
    "    \n",
    "    def forward(self, images: torch.Tensor, targets: dict=None):\n",
    "        return self.model(images, targets)\n",
    "    \n",
    "    def load(self, checkpoint_path: dict, key_name: str = \"model_state_dict\", map_location: str = \"cpu\"):\n",
    "        self.load_state_dict(torch.load(checkpoint_path, map_location=map_location)[key_name])\n",
    "    \n",
    "    def evaluate(self, dataset_root: str, device: torch.device|str, batch_size: int = 64):\n",
    "        num_cores = os.cpu_count()\n",
    "        print(f\"Number of CPU cores: {num_cores}\")\n",
    "        \n",
    "        metric = MeanAveragePrecision(\n",
    "        iou_type=\"bbox\",\n",
    "        class_metrics=True,\n",
    "        extended_summary=True)\n",
    "        \n",
    "        train_dataset = SSDLITEOBJDET_DATASET(dataset_root, split='train')\n",
    "        val_dataset = SSDLITEOBJDET_DATASET(dataset_root, split='val')\n",
    "        test_dataset = SSDLITEOBJDET_DATASET(dataset_root, split='test')\n",
    "        \n",
    "        # if device is a string\n",
    "        if isinstance(device, str):\n",
    "            device = device\n",
    "        else:\n",
    "            device = f\"{device.type}:{device.index}\"\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=device)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=device)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=device)\n",
    "        \n",
    "        # splits = ['train', 'val', 'test']\n",
    "        # loaders = [train_loader, val_loader, test_loader]\n",
    "        splits = ['val', 'test']\n",
    "        loaders = [val_loader, test_loader]\n",
    "        results = {}\n",
    "        for split, loader in zip(splits, loaders):\n",
    "            print(f\"Evaluating {split} set\")\n",
    "            self.eval()\n",
    "            metric.reset()\n",
    "            progress_bar = tqdm(loader, desc=f\"Evaluating {split} set\", unit=\"batch\")\n",
    "            with torch.no_grad():\n",
    "                for images, targets in progress_bar:\n",
    "                    images = list(image.to(device) for image in images)\n",
    "                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                    outputs = self(images)\n",
    "                    metric.update(outputs, targets)\n",
    "                results[split] =  metric.compute()\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD_MOBILENET_V3_Large(\n",
       "  (model): SSD(\n",
       "    (backbone): SSDLiteFeatureExtractorMobileNet(\n",
       "      (features): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "                (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "                (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "                (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "                (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (12): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "                (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (13): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (extra): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], clip=True, scales=[0.2, 0.35, 0.5, 0.65, 0.8, 0.95, 1.0], steps=None)\n",
       "    (head): SSDLiteHead(\n",
       "      (classification_head): SSDLiteClassificationHead(\n",
       "        (module_list): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(672, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(480, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(512, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3-4): 2 x Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(256, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(128, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (regression_head): SSDLiteRegressionHead(\n",
       "        (module_list): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3-4): 2 x Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transform): GeneralizedRCNNTransform(\n",
       "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       "        Resize(min_size=(320,), max_size=320, mode='bilinear')\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SSD_MOBILENET_V3_Large(num_classes_with_bg=MODEL_NUM_CLASSES)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.load(\"ssd_checkpoint/checkpoint_1.pth\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 4\n",
      "Evaluating val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val set: 100%|██████████| 22/22 [00:20<00:00,  1.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set: 100%|██████████| 22/22 [00:20<00:00,  1.08batch/s]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(DATASET_FOLDER_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Set device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Load the model\n",
    "    model = SSD_MOBILENET_V3_Large(num_classes_with_bg=MODEL_NUM_CLASSES)\n",
    "    model.to(device)\n",
    "    \n",
    "    train_dataset = SSDLITEOBJDET_DATASET(DATASET_FOLDER_PATH, 'train')\n",
    "    val_dataset = SSDLITEOBJDET_DATASET(DATASET_FOLDER_PATH, 'val')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=\"cuda:0\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=\"cuda:0\")\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = model.configure_optimizers(lr=0.0001, betas=(0.9, 0.999), weight_decay=0.001, eps=1e-08, fused=True)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        # Import tqdm for progress bar\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        \n",
    "        for _, (images, targets) in enumerate(train_bar):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = losses.detach().item()\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            # Update progress bar with current batch loss\n",
    "            train_bar.set_postfix(loss=batch_loss)\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        metric = MeanAveragePrecision()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                predictions = model(images)\n",
    "                metric.update(predictions, targets)\n",
    "        \n",
    "        map_result = metric.compute()\n",
    "        print(f\"Epoch {epoch+1} | Val mAP: {map_result['map']:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'ssd_mobilenet_v3_finetuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training function\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'split': split,\n",
    "            **split_metrics,\n",
    "            'time': f\"{elapsed:.1f}s\"\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nCompleted {split} split in {elapsed:.1f} seconds\")\n",
    "        print(f\"Split Metrics - mAP: {split_metrics['mAP']:.4f}, Precision: {split_metrics['Precision']:.4f}\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results).set_index('split')\n",
    "    numeric_cols = ['mAP', 'mAP_50', 'mAP_75', 'mAP_small', 'mAP_medium', \n",
    "                   'mAP_large', 'Recall', 'Precision', 'F1']\n",
    "    df[numeric_cols] = df[numeric_cols].applymap(lambda x: f\"{float(x):.4f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def evaluate_split(model, dataloader, device, metric):\n",
    "    \"\"\"Evaluate with batch-level progress\"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    \n",
    "    # Batch progress bar\n",
    "    batch_progress = tqdm(dataloader, \n",
    "                        desc=\"Processing batches\",\n",
    "                        leave=False,\n",
    "                        position=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in batch_progress:\n",
    "            # Move data to device\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Inference\n",
    "            predictions = model(images)\n",
    "            metric.update(predictions, targets)\n",
    "            \n",
    "            # Update progress description\n",
    "            batch_progress.set_postfix({\n",
    "                'current_mAP': f\"{metric.compute()['map'].item():.3f}\",\n",
    "                'batch_size': len(images)\n",
    "            })\n",
    "\n",
    "    # Final metrics\n",
    "    metrics = metric.compute()\n",
    "    \n",
    "    return {\n",
    "        'mAP': metrics['map'].item(),\n",
    "        'mAP_50': metrics['map_50'].item(),\n",
    "        'mAP_75': metrics['map_75'].item(),\n",
    "        'mAP_small': metrics['map_small'].item(),\n",
    "        'mAP_medium': metrics['map_medium'].item(),\n",
    "        'mAP_large': metrics['map_large'].item(),\n",
    "        'Recall': metrics['mar_100'].item(),\n",
    "        'Class_APs': metrics['classes'].cpu().numpy().round(4),\n",
    "        'Precision': metrics['precision'].cpu().numpy().mean().round(4),\n",
    "        'Recall': metrics['recall'].cpu().numpy().mean().round(4),\n",
    "        'F1': (2 * (metrics['precision'] * metrics['recall']) / \n",
    "              (metrics['precision'] + metrics['recall'] + 1e-16)).cpu().numpy().mean().round(4)\n",
    "    }\n",
    "\n",
    "def evaluate():\n",
    "    print(\"\\n🚀 Starting Comprehensive Evaluation\")\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"🔧 Using device: {device}\")\n",
    "    \n",
    "    # Model loading\n",
    "    print(\"\\n🔄 Loading model weights...\")\n",
    "    start_load = time.time()\n",
    "    model = SSD_MOBILENET_V3_Large(num_classes_with_bg=MODEL_NUM_CLASSES)\n",
    "\n",
    "    print(f\"✅ Model loaded in {time.time()-start_load:.1f}s\")\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\n📊 Starting evaluation on all splits...\")\n",
    "    metrics_df = evaluate_model(model, DATASET_FOLDER_PATH, device)\n",
    "    \n",
    "    # Results display\n",
    "    print(\"\\n🎯 Final Metrics Summary:\")\n",
    "    print(metrics_df[['mAP', 'mAP_50', 'mAP_75', 'Recall', 'Precision', 'F1', 'time']])\n",
    "    \n",
    "    print(\"\\n📈 Class-wise Performance:\")\n",
    "    class_df = pd.DataFrame(metrics_df['Class_APs'].tolist(), \n",
    "                          index=metrics_df.index).T\n",
    "    class_df.columns = metrics_df.index\n",
    "    print(class_df.round(4))\n",
    "    \n",
    "    print(\"\\n🏁 Evaluation complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
