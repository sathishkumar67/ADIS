{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.1.2\n",
      "    Uninstalling pip-24.1.2:\n",
      "      Successfully uninstalled pip-24.1.2\n",
      "Successfully installed pip-25.0.1\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.104-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting lmdb\n",
      "  Downloading lmdb-1.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.11.0a2)\n",
      "Collecting albucore==0.0.23 (from albumentations)\n",
      "  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.23->albumentations) (3.11.1)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n",
      "  Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (66 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24.4->albumentations) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.2->albumentations) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24.4->albumentations) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.4->albumentations) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24.4->albumentations) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24.4->albumentations) (2024.2.0)\n",
      "Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n",
      "Downloading albucore-0.0.23-py3-none-any.whl (14 kB)\n",
      "Downloading ultralytics-8.3.104-py3-none-any.whl (994 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.1/994.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lmdb-1.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Downloading simsimd-6.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (632 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: simsimd, lmdb, huggingface_hub, ultralytics-thop, opencv-python, albucore, ultralytics, torchmetrics, albumentations\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.29.0\n",
      "    Uninstalling huggingface-hub-0.29.0:\n",
      "      Successfully uninstalled huggingface-hub-0.29.0\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.10.0.84\n",
      "    Uninstalling opencv-python-4.10.0.84:\n",
      "      Successfully uninstalled opencv-python-4.10.0.84\n",
      "  Attempting uninstall: albucore\n",
      "    Found existing installation: albucore 0.0.19\n",
      "    Uninstalling albucore-0.0.19:\n",
      "      Successfully uninstalled albucore-0.0.19\n",
      "  Attempting uninstall: torchmetrics\n",
      "    Found existing installation: torchmetrics 1.6.1\n",
      "    Uninstalling torchmetrics-1.6.1:\n",
      "      Successfully uninstalled torchmetrics-1.6.1\n",
      "  Attempting uninstall: albumentations\n",
      "    Found existing installation: albumentations 1.4.20\n",
      "    Uninstalling albumentations-1.4.20:\n",
      "      Successfully uninstalled albumentations-1.4.20\n",
      "Successfully installed albucore-0.0.23 albumentations-2.0.5 huggingface_hub-0.30.2 lmdb-1.6.2 opencv-python-4.11.0.86 simsimd-6.2.1 torchmetrics-1.7.1 ultralytics-8.3.104 ultralytics-thop-2.0.14\n",
      "Cloning into 'ADIS'...\n",
      "remote: Enumerating objects: 1037, done.\u001b[K\n",
      "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 1037 (delta 29), reused 41 (delta 13), pack-reused 978 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1037/1037), 39.60 MiB | 37.68 MiB/s, done.\n",
      "Resolving deltas: 100% (525/525), done.\n",
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce442c1a7ab9490eb3bae4d7dbad5d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "balanced_dataset.zip:   0%|          | 0.00/7.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unzipping: 100%|██████████| 7.07G/7.07G [00:47<00:00, 149MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -U albumentations ultralytics huggingface_hub torchmetrics lmdb msgpack opencv-python \n",
    "!git clone https://github.com/sathishkumar67/ADIS.git\n",
    "!mv /kaggle/working/ADIS/* /kaggle/working/\n",
    "\n",
    "# necessary imports\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from utils import unzip_file\n",
    "\n",
    "REPO_ID = \"pt-sk/ADIS\" \n",
    "DATASET_NAME = \"balanced_dataset\"\n",
    "REPO_TYPE = \"dataset\"\n",
    "FILENAME_IN_REPO = f\"{DATASET_NAME}.zip\"\n",
    "LOCAL_DIR = os.getcwd()\n",
    "DATASET_PATH = f\"{LOCAL_DIR}/{FILENAME_IN_REPO}\"\n",
    "DATASET_FOLDER_PATH = f\"{LOCAL_DIR}/{DATASET_NAME}\"\n",
    "NUM_CLASSES = 10                                               \n",
    "CLASSES = ['Cat', 'Cattle', 'Chicken', 'Deer', 'Dog', 'Squirrel', 'Eagle', 'Goat', 'Rodents', 'Snake'] \n",
    "BACKGROUND_CLASS_ID = 0\n",
    "MODEL_NUM_CLASSES = NUM_CLASSES + 1     # 1 for background class\n",
    "\n",
    "# download the dataset and unzip it\n",
    "hf_hub_download(repo_id=REPO_ID, filename=FILENAME_IN_REPO, repo_type=REPO_TYPE, local_dir=LOCAL_DIR)\n",
    "unzip_file(DATASET_PATH, LOCAL_DIR)\n",
    "\n",
    "# remove dataset.zip\n",
    "os.remove(DATASET_PATH)\n",
    "\n",
    "# number of cores\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "import msgpack\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.detection import _utils as det_utils\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from typing import List, Tuple, Dict, Any # Import necessary types\n",
    "import lmdb\n",
    "import pickle\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDLITEOBJDET_DATASET(Dataset):\n",
    "    def __init__(self, root_dir: str, split: str, num_classes: int, img_size: int=320, dtype: torch.dtype=torch.float16, mode: str=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        self.root_dir, self.split, self.img_size, self.num_classes = root_dir, split.lower(), img_size, num_classes\n",
    "        self.current_dir = os.path.join(self.root_dir, self.split)\n",
    "        self.dtype, self.mode = dtype, mode.lower()\n",
    "        \n",
    "        # check if model is train or eval\n",
    "        if self.mode not in [\"train\", \"eval\"]:\n",
    "            raise ValueError(f\"Invalid mode: {self.mode}. Expected 'train' or 'eval'.\")\n",
    "        \n",
    "        # set interpolation method for resizing\n",
    "        self.interpolation = cv2.INTER_LANCZOS4 if self.mode == \"train\" else cv2.INTER_LINEAR\n",
    "\n",
    "        # Validate current directory\n",
    "        if not os.path.exists(self.current_dir):\n",
    "            raise FileNotFoundError(f\"{self.current_dir} does not exist.\")\n",
    "        elif not os.path.isdir(self.current_dir):\n",
    "            raise NotADirectoryError(f\"{self.current_dir} is not a directory.\")\n",
    "        \n",
    "        # check if the split directory is empty\n",
    "        if len(os.listdir(self.current_dir)) == 0:\n",
    "            raise ValueError(f\"The directory {self.current_dir} is empty.\")\n",
    "        \n",
    "        # get image and label files\n",
    "        self.image_files = sorted(\n",
    "            [f for f in os.listdir(self.current_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))],\n",
    "            key=lambda x: os.path.splitext(x)[0]\n",
    "        )\n",
    "        self.label_files = [os.path.splitext(f)[0] + '.txt' for f in self.image_files]\n",
    "\n",
    "        # Validate existence for ALL label files\n",
    "        for img_file, lbl_file in zip(self.image_files, self.label_files):\n",
    "            if not os.path.exists(os.path.join(self.current_dir, lbl_file)):\n",
    "                raise FileNotFoundError(f\"Label file missing for {img_file}\")\n",
    "            elif not os.path.exists(os.path.join(self.current_dir, img_file)):\n",
    "                raise FileNotFoundError(f\"Image file missing for {lbl_file}\")\n",
    "        \n",
    "        # get image and label file paths\n",
    "        self.image_files = [os.path.join(self.current_dir, f) for f in self.image_files]\n",
    "        self.label_files = [os.path.join(self.current_dir, f) for f in self.label_files]\n",
    "        \n",
    "        # check pair mismatches\n",
    "        for img_file, lbl_file in zip(self.image_files, self.label_files):\n",
    "            if os.path.splitext(img_file)[0] != os.path.splitext(lbl_file)[0]:\n",
    "                raise ValueError(f\"Image and label file names do not match: {img_file} and {lbl_file}\")\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        # print the no of items in the dataset\n",
    "        print(f\"Number of items in the dataset: {len(self.image_files)}\")\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, dict]:\n",
    "        img_path, label_path = self.image_files[idx], self.label_files[idx]\n",
    "\n",
    "        # Read image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        orig_width, orig_height, _ = image.shape\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size), interpolation=self.interpolation)\n",
    "        \n",
    "        # Convert image to tensor, normalize to [0, 1], then to required dtype\n",
    "        image = TF.to_tensor(image)\n",
    "        \n",
    "        # In label parsing\n",
    "        valid_boxes = []\n",
    "        valid_labels = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue  # Skip empty lines\n",
    "                    \n",
    "                parts = line.split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    cid, cx, cy, w, h = map(float, parts)\n",
    "                    if not (0 <= cx <= 1 and 0 <= cy <= 1 and 0 <= w <= 1 and 0 <= h <= 1):\n",
    "                        continue  # Skip invalid normalized values\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "                # Convert to absolute coordinates\n",
    "                xmin = max(0, (cx - w/2) * orig_width)\n",
    "                ymin = max(0, (cy - h/2) * orig_height)\n",
    "                xmax = min(orig_width, (cx + w/2) * orig_width)\n",
    "                ymax = min(orig_height, (cy + h/2) * orig_height)\n",
    "                \n",
    "                # Validate box dimensions\n",
    "                if xmax - xmin < 1 or ymax - ymin < 1:\n",
    "                    continue  # Skip degenerate boxes\n",
    "                    \n",
    "                valid_boxes.append([xmin, ymin, xmax, ymax])\n",
    "                valid_labels.append(int(cid) + 1)  # Verify class ID range\n",
    "\n",
    "            # Validate class IDs\n",
    "            if valid_labels and (max(valid_labels) >= self.num_classes or min(valid_labels) < 0):\n",
    "                raise ValueError(f\"Invalid class ID in {label_path}\")\n",
    "            \n",
    "        # Handle empty labels after all filtering\n",
    "        if len(valid_boxes) == 0:\n",
    "            return image.to(self.dtype), {\n",
    "                'boxes': torch.tensor([[0.0, 0.0, 1.0, 1.0]], dtype=self.dtype),\n",
    "                'labels': torch.zeros(0, dtype=torch.uint8)\n",
    "            }\n",
    "        else:    \n",
    "            # Scale boxes\n",
    "            scale_x, scale_y = self.img_size / orig_width, self.img_size / orig_height\n",
    "            boxes = torch.tensor(valid_boxes) * torch.tensor([scale_x, scale_y, scale_x, scale_y])\n",
    "            target = {\n",
    "                'boxes': self.normalize(boxes).to(self.dtype) if self.mode == \"train\" else boxes.to(self.dtype),\n",
    "                'labels': torch.tensor(valid_labels, dtype=torch.uint8)\n",
    "            }\n",
    "            return image.to(self.dtype), target\n",
    "        \n",
    "    def normalize(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        # Normalize boxes to [0, 1] range\n",
    "        for box in boxes:\n",
    "            for i in range(4):\n",
    "                box[i] = box[i].item() / self.img_size\n",
    "        return boxes\n",
    "    \n",
    "    def denormalize(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        # Denormalize boxes to original size\n",
    "        for box in boxes:\n",
    "            for i in range(4):\n",
    "                box[i] = box[i].item() * self.img_size\n",
    "        return boxes\n",
    "\n",
    "\n",
    "def ssdlite_collate_fn(batch: List[Tuple[torch.Tensor, Dict[str, torch.Tensor]]]) -> Tuple[torch.Tensor, List[Dict[str, torch.Tensor]]]:\n",
    "    \"\"\"\n",
    "    Custom collate function for the SSDLITEOBJDET_DATASET.\n",
    "\n",
    "    It stacks the images along a new batch dimension but keeps the targets\n",
    "    (dictionaries containing 'boxes' and 'labels') as a list, as the number\n",
    "    of objects varies per image.\n",
    "\n",
    "    Args:\n",
    "        batch: A list where each element is a tuple returned by\n",
    "               `SSDLITEOBJDET_DATASET.__getitem__`.\n",
    "               Each tuple contains:\n",
    "               - image (torch.Tensor): An image tensor of shape [C, H, W].\n",
    "               - target (dict): A dictionary with keys 'boxes' (Tensor[N, 4])\n",
    "                                and 'labels' (Tensor[N]), where N is the\n",
    "                                number of objects in the image.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - images (torch.Tensor): A batch of images stacked along dimension 0,\n",
    "                                 shape [B, C, H, W], where B is batch size.\n",
    "        - targets (List[dict]): A list of target dictionaries. The length\n",
    "                                 of the list is B (batch size). Each dictionary\n",
    "                                 corresponds to an image in the `images` batch.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "\n",
    "    # Separate images and targets from the input batch list\n",
    "    for image_tensor, target_dict in batch:\n",
    "        images.append(image_tensor)\n",
    "        targets.append(target_dict)\n",
    "\n",
    "    # Stack the image tensors along a new dimension (dim=0) to create the batch\n",
    "    # This assumes all images were resized to the same size in __getitem__\n",
    "    images_batch = torch.stack(images, dim=0)\n",
    "\n",
    "    # The targets remain as a list of dictionaries\n",
    "    # The object detection model's forward pass or the loss function\n",
    "    # will typically iterate through this list.\n",
    "    # Each dictionary in the list corresponds to an image in the batch.\n",
    "    return images_batch, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SSDLITEOBJDET_DATASET(\n",
    "    root_dir=DATASET_FOLDER_PATH,\n",
    "    split=\"val\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    img_size=320,\n",
    "    dtype=torch.float16,\n",
    "    mode=\"eval\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedSSDLITEOBJDET_DATASET(Dataset):\n",
    "    def __init__(self, dataset_class :SSDLITEOBJDET_DATASET, \n",
    "                root_dir: str, \n",
    "                split: str, \n",
    "                num_classes: int, \n",
    "                img_size: int=320, \n",
    "                dtype: torch.dtype=torch.float32, \n",
    "                mode: str=\"train\",\n",
    "                lmdb_path: str = None,\n",
    "                map_size: int=1099511627776) -> None:\n",
    "        super().__init__()\n",
    "        self.root_dir, self.split, self.img_size, self.num_classes = root_dir, split.lower(), img_size, num_classes\n",
    "        self.mode, self.dtype = mode.lower(), dtype\n",
    "        self.dataset_class = dataset_class\n",
    "        self.map_size = map_size\n",
    "        self.lmdb_path = lmdb_path if lmdb_path else os.path.join(self.root_dir, f\"{self.split}_cache\")\n",
    "        \n",
    "        # preprocess the dataset and cache it in lmdb\n",
    "        self.preprocess_dataset()\n",
    "        \n",
    "        self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False)\n",
    "        with self.env.begin() as txn:\n",
    "            self.length = txn.stat()['entries']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with self.env.begin() as txn:\n",
    "            data = msgpack.unpackb(txn.get(str(idx).encode()))\n",
    "        img = torch.frombuffer(data[0], dtype=self.dtype).view(3, self.img_size, self.img_size) # channnels = 3 because of RGB\n",
    "        boxes = torch.frombuffer(data[1], dtype=self.dtype).view(-1, 4)\n",
    "        labels = torch.frombuffer(data[2], dtype=torch.uint8).view(-1)\n",
    "        return img, {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "            # data = txn.get(str(idx).encode())\n",
    "        # return pickle.loads(data)\n",
    "    \n",
    "    def preprocess_dataset(self) -> None:\n",
    "        dataset = self.dataset_class(self.root_dir, self.split, self.num_classes, self.img_size, self.dtype, self.mode)\n",
    "        env = lmdb.open(self.lmdb_path, map_size=self.map_size)  # 1TB\n",
    "        \n",
    "        with env.begin(write=True) as txn:\n",
    "            for idx in tqdm(range(len(dataset))):\n",
    "                image, target = dataset[idx]\n",
    "                # Serialize and store\n",
    "                # txn.put(\n",
    "                #     str(idx).encode(),\n",
    "                #     pickle.dumps((image, target))\n",
    "                # )\n",
    "                # serialize with msgpack (faster than pickle)\n",
    "                txn.put(str(idx).encode(), msgpack.packb((\n",
    "                image.numpy().tobytes(),\n",
    "                target['boxes'].numpy().tobytes(),\n",
    "                target['labels'].numpy().tobytes()\n",
    "            )))\n",
    "        shutil.rmtree(os.path.join(self.root_dir, self.split))\n",
    "        del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class AsyncPrefetcher:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.pool = ThreadPoolExecutor(max_workers=4)\n",
    "        self.futures = []\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Prefetch next 4 batches\n",
    "        for _ in range(4):\n",
    "            self.futures.append(self.pool.submit(next, self.dataloader))\n",
    "        while True:\n",
    "            yield self.futures.pop(0).result()\n",
    "            self.futures.append(self.pool.submit(next, self.dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "\n",
    "# Bypass CPU copy using NVIDIA Magnum IO\n",
    "torch.utils.data.DataLoader(..., direct_io=True)\n",
    "\n",
    "# Mount filesystem with GPU Direct support\n",
    "mount -t gpfs /dev/nvme0n1 /datasets\n",
    "\n",
    "# fused batch collation\n",
    "@torch.jit.script\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return torch.stack(images, dim=0, memory_format=torch.channels_last), targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. dataloader configuration\n",
    "2. parallelize label parsing\n",
    "3. use nvidia dali for gpu based dataloading\n",
    "4. use concurrent features in preprocess dataset to speed up the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the dataset: 2390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2390/2390 [00:30<00:00, 79.13it/s] \n"
     ]
    }
   ],
   "source": [
    "val_dataset = CachedSSDLITEOBJDET_DATASET(SSDLITEOBJDET_DATASET,\n",
    "                                        DATASET_FOLDER_PATH,\n",
    "                                        \"val\", \n",
    "                                        MODEL_NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "class SSD_MOBILENET_V3_Large(nn.Module):\n",
    "    def __init__(self, num_classes_with_bg:int) -> None:\n",
    "        super(SSD_MOBILENET_V3_Large, self).__init__()\n",
    "        self.num_classes_with_bg = num_classes_with_bg\n",
    "        self.model = ssdlite320_mobilenet_v3_large(weights='COCO_V1', weights_backbone=\"DEFAULT\") \n",
    "        self.model.head.classification_head = SSDLiteClassificationHead(\n",
    "            in_channels=det_utils.retrieve_out_channels(self.model.backbone, (320, 320)),\n",
    "            num_anchors=self.model.anchor_generator.num_anchors_per_location(),\n",
    "            num_classes=self.num_classes_with_bg,\n",
    "            norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.03)\n",
    "        )\n",
    "        self.model.detections_per_img = 100\n",
    "    \n",
    "    def configure_optimizers(self, lr: float = 0.0001, betas: Tuple[float, float] = (0.9, 0.999), weight_decay: float = 0.0001, eps: float = 1e-08, fused: bool = True) -> torch.optim.Optimizer:        \n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for _, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for _, p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        # Create AdamW optimizer and use the fused version if available \n",
    "        return torch.optim.AdamW([{'params': decay_params, 'weight_decay': weight_decay},\n",
    "                                    {'params': nodecay_params, 'weight_decay': 0.0}], \n",
    "                                    lr=lr, \n",
    "                                    betas=betas, \n",
    "                                    eps=eps, \n",
    "                                    fused=fused)\n",
    "    \n",
    "    def forward(self, images: torch.Tensor, targets: dict=None):\n",
    "        return self.model(images, targets)\n",
    "    \n",
    "    def load(self, checkpoint_path: dict, key_name: str = \"model_state_dict\", map_location: str = \"cpu\"):\n",
    "        self.load_state_dict(torch.load(checkpoint_path, map_location=map_location)[key_name])\n",
    "    \n",
    "    def evaluate(self, dataset_root: str, device: torch.device|str, batch_size: int = 64):\n",
    "        num_cores = os.cpu_count()\n",
    "        print(f\"Number of CPU cores: {num_cores}\")\n",
    "        \n",
    "        metric = MeanAveragePrecision(\n",
    "        iou_type=\"bbox\",\n",
    "        class_metrics=True,\n",
    "        extended_summary=True)\n",
    "        \n",
    "        train_dataset = SSDLITEOBJDET_DATASET(dataset_root, split='train')\n",
    "        val_dataset = SSDLITEOBJDET_DATASET(dataset_root, split='val')\n",
    "        test_dataset = SSDLITEOBJDET_DATASET(dataset_root, split='test')\n",
    "        \n",
    "        # if device is a string\n",
    "        if isinstance(device, str):\n",
    "            device = device\n",
    "        else:\n",
    "            device = f\"{device.type}:{device.index}\"\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=device)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=device)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=device)\n",
    "        \n",
    "        # splits = ['train', 'val', 'test']\n",
    "        # loaders = [train_loader, val_loader, test_loader]\n",
    "        splits = ['val', 'test']\n",
    "        loaders = [val_loader, test_loader]\n",
    "        results = {}\n",
    "        for split, loader in zip(splits, loaders):\n",
    "            print(f\"Evaluating {split} set\")\n",
    "            self.eval()\n",
    "            metric.reset()\n",
    "            progress_bar = tqdm(loader, desc=f\"Evaluating {split} set\", unit=\"batch\")\n",
    "            with torch.no_grad():\n",
    "                for images, targets in progress_bar:\n",
    "                    images = list(image.to(device) for image in images)\n",
    "                    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                    outputs = self(images)\n",
    "                    metric.update(outputs, targets)\n",
    "                results[split] =  metric.compute()\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD_MOBILENET_V3_Large(\n",
       "  (model): SSD(\n",
       "    (backbone): SSDLiteFeatureExtractorMobileNet(\n",
       "      (features): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "                (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "                (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "                (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "                (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "                (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "                (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "                (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (12): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "                (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (13): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "                (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "                (2): Hardswish()\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): ReLU()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (extra): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], clip=True, scales=[0.2, 0.35, 0.5, 0.65, 0.8, 0.95, 1.0], steps=None)\n",
       "    (head): SSDLiteHead(\n",
       "      (classification_head): SSDLiteClassificationHead(\n",
       "        (module_list): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(672, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(480, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(512, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3-4): 2 x Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(256, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(128, 66, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (regression_head): SSDLiteRegressionHead(\n",
       "        (module_list): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3-4): 2 x Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU6(inplace=True)\n",
       "            )\n",
       "            (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transform): GeneralizedRCNNTransform(\n",
       "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       "        Resize(min_size=(320,), max_size=320, mode='bilinear')\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SSD_MOBILENET_V3_Large(num_classes_with_bg=MODEL_NUM_CLASSES)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.load(\"ssd_checkpoint/checkpoint_1.pth\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 4\n",
      "Evaluating val set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating val set: 100%|██████████| 22/22 [00:20<00:00,  1.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating test set: 100%|██████████| 22/22 [00:20<00:00,  1.08batch/s]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(DATASET_FOLDER_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Set device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    # Load the model\n",
    "    model = SSD_MOBILENET_V3_Large(num_classes_with_bg=MODEL_NUM_CLASSES)\n",
    "    model.to(device)\n",
    "    \n",
    "    train_dataset = SSDLITEOBJDET_DATASET(DATASET_FOLDER_PATH, 'train')\n",
    "    val_dataset = SSDLITEOBJDET_DATASET(DATASET_FOLDER_PATH, 'val')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=\"cuda:0\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=num_cores, pin_memory=True, pin_memory_device=\"cuda:0\")\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = model.configure_optimizers(lr=0.0001, betas=(0.9, 0.999), weight_decay=0.001, eps=1e-08, fused=True)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        # Import tqdm for progress bar\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        \n",
    "        for _, (images, targets) in enumerate(train_bar):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = losses.detach().item()\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            # Update progress bar with current batch loss\n",
    "            train_bar.set_postfix(loss=batch_loss)\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        metric = MeanAveragePrecision()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                predictions = model(images)\n",
    "                metric.update(predictions, targets)\n",
    "        \n",
    "        map_result = metric.compute()\n",
    "        print(f\"Epoch {epoch+1} | Val mAP: {map_result['map']:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'ssd_mobilenet_v3_finetuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training function\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'split': split,\n",
    "            **split_metrics,\n",
    "            'time': f\"{elapsed:.1f}s\"\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nCompleted {split} split in {elapsed:.1f} seconds\")\n",
    "        print(f\"Split Metrics - mAP: {split_metrics['mAP']:.4f}, Precision: {split_metrics['Precision']:.4f}\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results).set_index('split')\n",
    "    numeric_cols = ['mAP', 'mAP_50', 'mAP_75', 'mAP_small', 'mAP_medium', \n",
    "                   'mAP_large', 'Recall', 'Precision', 'F1']\n",
    "    df[numeric_cols] = df[numeric_cols].applymap(lambda x: f\"{float(x):.4f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def evaluate_split(model, dataloader, device, metric):\n",
    "    \"\"\"Evaluate with batch-level progress\"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    \n",
    "    # Batch progress bar\n",
    "    batch_progress = tqdm(dataloader, \n",
    "                        desc=\"Processing batches\",\n",
    "                        leave=False,\n",
    "                        position=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in batch_progress:\n",
    "            # Move data to device\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Inference\n",
    "            predictions = model(images)\n",
    "            metric.update(predictions, targets)\n",
    "            \n",
    "            # Update progress description\n",
    "            batch_progress.set_postfix({\n",
    "                'current_mAP': f\"{metric.compute()['map'].item():.3f}\",\n",
    "                'batch_size': len(images)\n",
    "            })\n",
    "\n",
    "    # Final metrics\n",
    "    metrics = metric.compute()\n",
    "    \n",
    "    return {\n",
    "        'mAP': metrics['map'].item(),\n",
    "        'mAP_50': metrics['map_50'].item(),\n",
    "        'mAP_75': metrics['map_75'].item(),\n",
    "        'mAP_small': metrics['map_small'].item(),\n",
    "        'mAP_medium': metrics['map_medium'].item(),\n",
    "        'mAP_large': metrics['map_large'].item(),\n",
    "        'Recall': metrics['mar_100'].item(),\n",
    "        'Class_APs': metrics['classes'].cpu().numpy().round(4),\n",
    "        'Precision': metrics['precision'].cpu().numpy().mean().round(4),\n",
    "        'Recall': metrics['recall'].cpu().numpy().mean().round(4),\n",
    "        'F1': (2 * (metrics['precision'] * metrics['recall']) / \n",
    "              (metrics['precision'] + metrics['recall'] + 1e-16)).cpu().numpy().mean().round(4)\n",
    "    }\n",
    "\n",
    "def evaluate():\n",
    "    print(\"\\n🚀 Starting Comprehensive Evaluation\")\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"🔧 Using device: {device}\")\n",
    "    \n",
    "    # Model loading\n",
    "    print(\"\\n🔄 Loading model weights...\")\n",
    "    start_load = time.time()\n",
    "    model = SSD_MOBILENET_V3_Large(num_classes_with_bg=MODEL_NUM_CLASSES)\n",
    "\n",
    "    print(f\"✅ Model loaded in {time.time()-start_load:.1f}s\")\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\n📊 Starting evaluation on all splits...\")\n",
    "    metrics_df = evaluate_model(model, DATASET_FOLDER_PATH, device)\n",
    "    \n",
    "    # Results display\n",
    "    print(\"\\n🎯 Final Metrics Summary:\")\n",
    "    print(metrics_df[['mAP', 'mAP_50', 'mAP_75', 'Recall', 'Precision', 'F1', 'time']])\n",
    "    \n",
    "    print(\"\\n📈 Class-wise Performance:\")\n",
    "    class_df = pd.DataFrame(metrics_df['Class_APs'].tolist(), \n",
    "                          index=metrics_df.index).T\n",
    "    class_df.columns = metrics_df.index\n",
    "    print(class_df.round(4))\n",
    "    \n",
    "    print(\"\\n🏁 Evaluation complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
