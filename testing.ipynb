{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U albumentations ultralytics huggingface_hub\n",
    "!git clone https://github.com/sathishkumar67/ADIS.git\n",
    "!mv /kaggle/working/ADIS/* /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from utils import unzip_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"pt-sk/Animal_Intrusion\" \n",
    "FILENAME_IN_REPO = \"dataset.zip\"\n",
    "LOCAL_DIR = os.getcwd()\n",
    "REPO_TYPE = \"dataset\"\n",
    "DATASET_PATH = f\"{LOCAL_DIR}/{FILENAME_IN_REPO}\"\n",
    "DATASET_FOLDER_PATH = f\"{LOCAL_DIR}/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ADIS\t       LICENSE\t\t  testing.ipynb       'yolo11n bohb tune'\n",
      " blocks.py     model.py\t\t  trainer.py\t      'yolo11s bohb tune'\n",
      " config        __pycache__\t  utils.py\n",
      " dataset       README.md\t  validator.py\n",
      " detector.py   requirements.txt  'yolo11m bohb tune'\n"
     ]
    }
   ],
   "source": [
    "# download the dataset and unzip it\n",
    "hf_hub_download(repo_id=REPO_ID, filename=FILENAME_IN_REPO, repo_type=REPO_TYPE, local_dir=LOCAL_DIR)\n",
    "unzip_file(DATASET_PATH, LOCAL_DIR)\n",
    "\n",
    "# remove dataset.zip\n",
    "os.remove(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 11                    # old class count                           \n",
    "CLASSES = ['Cat', 'Cattle', 'Chicken', 'Deer', 'Dog', \"Duck\", 'Eagle', 'Goat', 'Rodents', 'Snake', 'Squirrel'] # old classes\n",
    "BACKGROUND_CLASS_ID = 0\n",
    "MODEL_NUM_CLASSES = NUM_CLASSES + 1     # old class count + 1 for background class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "from torchvision.models.detection import _utils as det_utils\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.detection import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ssdlite320_mobilenet_v3_large(weights='COCO_V1')\n",
    "in_channels = det_utils.retrieve_out_channels(model.backbone, (320, 320))\n",
    "num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.03)\n",
    "model.head.classification_head = SSDLiteClassificationHead(in_channels, num_anchors, MODEL_NUM_CLASSES, norm_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, root_dir, split, img_size=320):\n",
    "        self.root = os.path.join(root_dir, split)\n",
    "        self.img_dir = os.path.join(self.root, 'images')\n",
    "        self.label_dir = os.path.join(self.root, 'labels')\n",
    "        self.img_size = img_size\n",
    "        self.image_files = [f for f in os.listdir(self.img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        orig_width, orig_height = image.size\n",
    "        \n",
    "        # Load annotations\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                cid, cx, cy, w, h = map(float, line.strip().split())\n",
    "                \n",
    "                # Convert YOLO format to absolute coordinates\n",
    "                xmin = (cx - w/2) * orig_width\n",
    "                ymin = (cy - h/2) * orig_height\n",
    "                xmax = (cx + w/2) * orig_width\n",
    "                ymax = (cy + h/2) * orig_height\n",
    "                \n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(int(cid) + 1)  # Add 1 for background class\n",
    "\n",
    "        # Convert to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        \n",
    "        # Resize\n",
    "        image = TF.resize(image, (self.img_size, self.img_size))\n",
    "        scale_x = self.img_size / orig_width\n",
    "        scale_y = self.img_size / orig_height\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) * torch.tensor([scale_x, scale_y, scale_x, scale_y])\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_classes = NUM_CLASSES + 1  # old class count + 1 for background class\n",
    "    train_dataset = YOLODataset(DATASET_FOLDER_PATH, 'train')\n",
    "    val_dataset = YOLODataset(DATASET_FOLDER_PATH, 'val')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.AdamW(params, lr=0.0001, weight_decay=0.0005)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        # Import tqdm for progress bar\n",
    "        from tqdm import tqdm\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_bar):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = losses.item()\n",
    "            total_loss += batch_loss\n",
    "            \n",
    "            # Update progress bar with current batch loss\n",
    "            train_bar.set_postfix(loss=batch_loss)\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        metric = MeanAveragePrecision()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                predictions = model(images)\n",
    "                metric.update(predictions, targets)\n",
    "        \n",
    "        map_result = metric.compute()\n",
    "        print(f\"Epoch {epoch+1} | Val mAP: {map_result['map']:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'ssd_mobilenet_v3_finetuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   4%|‚ñç         | 21/534 [00:06<01:58,  4.31batch/s, loss=6.11]"
     ]
    }
   ],
   "source": [
    "# Run the training function\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
